---
ID: 3986
post_title: ""
author: UfU
post_excerpt: ""
layout: post
permalink: >
  http://universalflowuniversity.com/ufu/3986-2/
published: true
post_date: 2018-01-03 00:32:31
---
<div class="col-md-6 col-md-offset-3 all-content">
<!--&lt;!&ndash;<div class='col-md-12 all-content'>&ndash;&gt;-->

<div><p class="c57"><span class="c7 c51">A Year in Computer Vision: </span><span class="c27 c7 c51">The M Tank, 2017</span></p></div>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p><a id="t.5827d31479002d9af0fec6b8e6e4de6b20c8710d"></a><a id="t.0"></a>
<table class="c58">
    <tbody>
    <tr class="c48">
        <td class="c55" colspan="1" rowspan="1"><h1 class="c34 c37" id="h.ezv3dsj3a2mz"><span class="c32 c35"></span></h1>

            <h1 class="c37 main-heading-kinda" id="heading-a-year-in-cv"><span class="c35 c32">A Year in Computer Vision</span></h1>

            <p class="c9 c19"><span class="c20 c12 c6"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c1 c9"><span class="c8 c33"></span></p>

            <p class="c10"><span class="c5">Edited for The M Tank by</span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10"><span class="c20 c12 c6">Benjamin F. Duffy </span></p>

            <p class="c10"><span class="c5">&amp;</span></p>

            <p class="c10"><span class="c20 c12 c6">Daniel R. Flynn</span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c1 c9"><span class="c5"></span></p>

            <p class="c1 c9"><span class="c5"></span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10 c9"><span class="c32 c6 c54"></span></p>

            <p class="c10 c9"><span class="c5"></span></p>

            <p class="c10"><span class="c54 c12 c56">The M Tank</span></p>

            <!--<p class="c9 c10"><span class="c5"></span></p>-->

            <!--<p class="c10"><span class="c12 c6">Website</span><span class="c5">: http://themtank.org/</span></p>-->

            <!--<p class="c10 c9"><span class="c5"></span></p>-->

            <!--<p class="c10"><span class="c6 c12">Contact</span><span class="c5">: daniel@themtank.com</span></p>-->

            <p class="c9 c39"><span class="c5"></span></p></td>
    </tr>
    </tbody>
</table>

    <!--<br>-->
    <br>
<p class="c1 c9" style="text-align: center"><span class="c5">Also on Medium: <a href="https://medium.com/@info_84181/a-year-in-computer-vision-part-1-of-4-eaeb040b6f46">Part 1</a>, <a href="https://medium.com/@info_84181/a-year-in-computer-vision-part-2-of-4-893e18e12be0">Part 2</a>, <a href="https://medium.com/@info_84181/a-year-in-computer-vision-part-3-of-4-861216d71607">Part 3</a>, <a href="https://medium.com/@info_84181/a-year-in-computer-vision-part-4-of-4-515c61d41a00">Part 4</a></span>
           </p>

<!--<p class="c1 c9"><span class="c5"></span></p><a id="t.1f747a6e414a2d42cdbfa2e72c33882655546e4b"></a><a id="t.1"></a>-->

<h1 class="c15 main-heading" id="heading-introduction"><span class="c20 c12 c25">Introduction</span></h1>

<p class="c1"><span class="c6">Computer Vision typically refers to the scientific discipline of giving machines the ability of sight, or perhaps more colourfully, enabling machines to visually analyse their environments and the stimuli within them. This process typically involves the evaluation of an image, images or video. </span><span class="c6">The British Machine Vision Association (BMVA) defines Computer Vision as </span><span class="c6">“</span><span class="c7 c6 c26 c14">the automatic extraction, analysis and </span><span class="c7 c6 c26 c14 c22">understanding</span><span class="c7 c6 c26 c14">&nbsp;of useful information from a single image or a sequence of images.</span><span class="c6 c26 c14">”</span><sup class="c6 c12"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup>
</p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1"><span class="c6 c26 c14">The term </span><span class="c7 c6 c26 c14">understanding</span><span class="c29 c6 c26 c14">&nbsp;provides an interesting counterpoint to an otherwise mechanical definition of vision, one which serves to demonstrate both the significance and complexity of the Computer Vision field. True understanding of our environment is not achieved through visual representations alone. Rather, visual cues travel through the optic nerve to the primary visual cortex and are interpreted by the brain, in a highly stylised sense. The interpretations drawn from this sensory information encompass the near-totality of our natural programming and subjective experiences, i.e. how evolution has wired us to survive and what we learn about the world throughout our lives. </span>
</p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1"><span class="c6 c26 c14">In this respect, </span><span class="c7 c6 c26 c14">vision</span><span class="c6 c26 c14">&nbsp;only relates to the transmission of images for interpretation; while </span><span class="c7 c6 c26 c14">computing </span><span class="c6 c26 c14">said images is more analogous to thought </span><span class="c6 c14">or cognition</span><span class="c6 c26 c14">, drawing on a multitude of the brain’s faculties. Hence, many believe that Computer Vision, a true understanding of visual environments and their contexts, </span><span class="c6 c26 c14">paves the way for future iterations of Strong Artificial Intelligence, due to its cross-domain mastery</span><span class="c29 c6 c26 c14">. </span></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1"><span class="c29 c6 c26 c14">However, put down the pitchforks as we’re still very much in the embryonic stages of this fascinating field. This piece simply aims to shed some light on 2016’s biggest Computer Vision advancements. And hopefully ground some of these advancements in a healthy mix of expected near-term societal-interactions and, where applicable, tongue-in-cheek prognostications of the end of life as we know it.</span>
</p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1"><span class="c6 c26 c14">While o</span><span class="c5">ur work is always written to be as accessible as possible, sections within this particular piece may be oblique at times due to the subject matter. We do provide rudimentary definitions throughout, however, these only convey a facile understanding of key concepts. In keeping our focus on work produced in 2016, often omissions are made in the interest of brevity.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">One such glaring omission relates to the functionality of Convolutional Neural Networks (hereafter CNNs or ConvNets), which are ubiquitous within the field of Computer Vision. The success of</span><span class="c6">&nbsp;AlexNet</span>
    <sup class="c6 c12" id="#ftnt_ref2"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c6">&nbsp;in 201</span><span class="c6">2, a CNN architecture which blindsided ImageNet competitors, proved instigator of a de facto revolution within the field, with numerous researchers adopting neural network-based approaches as part of Computer Vision’s new period of ‘normal science’.</span><sup class="c6 c12"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c5">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">Over four years later and CNN variants still make up the bulk of new neural network architectures for vision tasks, with researchers reconstructing them like legos; a working testament to the power of both open source information and Deep Learning. However, an explanation of CNNs could easily span several postings and is best left to those with a deeper expertise on the subject and an affinity for making the complex understandable. </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">For casual readers who wish to gain a quick grounding before proceeding we recommend the first two resources below. For those who wish to go further still, we have ordered the resources below to facilitate that:</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_240e30rf1rxr-0 start">
    <li class="c2 c13 c40"><h1 id="h.hftfmj325246" style="display:inline"><span class="c12 c6 c14">What a Deep Neural Network thinks about your #selfie</span><span class="c6 c14">&nbsp;</span><span class="c32 c6 c14">from Andrej Karpathy is one of our favourites for helping people understand the applications and functionalities behind CNNs.</span><sup class="c6 c12"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c5 c14">&nbsp;</span></h1></li>
    <li class="c40 c2 c13"><h1 id="h.mqa1vkna84m6" style="display:inline"><span class="c12 c6">Quora</span><span class="c6">: “what is a convolutional neural network?” - </span><span class="c32 c6">Has no shortage of great links and explanations. Particularly suited to those with </span><span class="c32 c6 c22">no prior understanding</span><span class="c32 c6">.</span><sup class="c6 c12"><a href="#ftnt5" id="ftnt_ref5">[5]</a></sup></h1></li>
    <li class="c40 c2 c13"><h1 id="h.6ccsyaqqg7md" style="display:inline"><span class="c12 c6">CS231n: Convolutional Neural Networks for Visual Recognition </span><span class="c12 c6">from Stanford University is an excellent resource for more depth.</span><sup class="c6 c12"><a href="#ftnt6" id="ftnt_ref6">[6]</a></sup></h1></li>
    <li class="c1 c2 c13"><span class="c12 c6">Deep Learning</span><span class="c6">&nbsp;(Goodfellow, Bengio &amp; Courville, 2016) provides detailed explanations of CNN features and functionality in Chapter 9. The textbook has been kindly made available for free in HTML format by the authors.</span><sup class="c6 c12"><a href="#ftnt7" id="ftnt_ref7">[7]</a></sup></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">For those wishing to understand more about Neural Networks and Deep Learning in general we suggest: </span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_kskz43o1igm-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Neural Networks and Deep Learning </span><span class="c6">(Nielsen, 2017) is a free online textbook which provides the reader with a really intuitive understanding of the complexities of Neural Networks and Deep Learning. Even just completing </span><span class="c6 c22">chapter one</span><span class="c6">&nbsp;should greatly illuminate the subject matter of this piece for first-timers.</span><sup class="c6 c12"><a href="#ftnt8" id="ftnt_ref8">[8]</a></sup></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">As a whole this piece is disjointed and spasmodic, a reflection of the authors’ excitement and the spirit in which it was intended to be utilised, </span><span class="c6">s</span><span class="c5">ection by section. Information is partitioned using our own heuristics and judgements, a necessary compromise due to the cross-domain influence of much of the work presented.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">We hope that readers benefit from our aggregation of the information here to further their own knowledge, regardless of previous experience.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">From all our contributors,</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"><img alt="" src="images/Signature-transparent.png" style="/*width: 624.00px; height: 290.67px;  transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);*/" title="">
</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

    <p class="c1"><span class="c5">The M Tank</span></p>

<h1 class="c15 main-heading" id="heading-part-one"><span class="c25">Part One: Classification/Localisation, Object Detection, Object Tracking</span></h1>

<h1 class="c15 main-heading" id="heading-classification"><span class="c20 c12 c25">Classification/Localisation</span></h1>

<p class="c1"><span class="c6">The task of classification, when it relates to images, </span><span class="c6">generally</span><span class="c6 c28">&nbsp;</span><span class="c6">refers to assigning a label to the whole image, e.g. ‘cat’. Assuming this, </span><span class="c6">Localisation </span><span class="c6">may then refer to finding where the object is in said image, </span><span class="c6">usually</span><span class="c6">&nbsp;denoted by the output of some form of bounding box around the object. Current classification/localisation techniques on</span><span class="c6">&nbsp;ImageNet</span><sup class="c6 c12"><a href="#ftnt9" id="ftnt_ref9">[9]</a></sup><span class="c6">&nbsp;have likely surpassed an ensemble of trained humans.</span><sup class="c6 c12"><a href="#ftnt10" id="ftnt_ref10">[10]</a></sup><span class="c6">&nbsp;</span><span class="c5">For this reason, we place greater emphasis on subsequent sections of the blog. </span></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c10"><span class="c12 c6 c26 c14">Figure 1</span><span class="c6 c26 c14 c29">: Computer Vision Tasks</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/     display: block; margin: 0 auto; /*margin: 0.00px 0.00px; border: 0.00px solid #000000; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); /*width: 624.00px; height: 223.90px;*/">
    <img class="border cntr" alt="" src="images/c-image12.jpg" style="width: 90%; max-width: 624px; /*width: 624.00px; height: 259.25px; margin-left: 0.00px; margin-top: -35.35px;  margin: 0 auto;transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="">
</span></p>

<p class="c1"><span class="c3 c12 c14 c26">Source</span><span class="c3 c26 c14">: Fei-Fei Li, Andrej Karpathy &amp; Justin Johnson (2016) cs231n, Lecture 8 - Slide 8, </span>
    <span class="c7 c3 c26 c14">Spatial Localization and Detection</span><span class="c3 c26 c14">&nbsp;(01/02/2016). Available: </span>
    <a class="a-word-wrap" href="http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf">http://cs231n.stanford.edu/slides/2016/winter1516_lecture8.pdf</a>
    <span class="c3 c26 c14">&nbsp;</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">However, the introduction of larger datasets with an increased number of classes</span><sup class="c6 c12"><a href="#ftnt11" id="ftnt_ref11">[11]</a></sup><span class="c6">&nbsp;will likely provide new metrics for progress</span><span class="c6">&nbsp;in the near future</span><span class="c6">. On that point, François Chollet, the creator of </span><span class="c6">Keras</span><span class="c6">,</span><sup class="c6 c12"><a href="#ftnt12" id="ftnt_ref12">[12]</a></sup><span class="c6">&nbsp;has applied new techniques, including the popular architecture Xception, to an internal google dataset with over 350 million multi-label images containing 17,000 classes.</span>
    <sup class="c6 c12"><a href="#ftnt13" id="ftnt_ref13">[13]</a></sup><span class="c6 c45">,</span><sup class="c6 c12"><a href="#ftnt14" id="ftnt_ref14">[14]</a></sup>
    <span class="c5">&nbsp;</span></p>
<br>
<p class="c10"><span class="c12 c6 c26 c14">Figure 2</span><span class="c29 c6 c26 c14">: Classification/Localisation results from ILSVRC (2010-2016)</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height:266px;">
    <img class="border cntr" alt="" src="images/image19.png" style="width: 95%; max-width: 624px; /*width: 624.00px; height: 266px; /*margin-left: 0.00px; margin-top: -81.15px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c3">: ImageNet Large Scale Visual Recognition Challenge (ILSVRC). The change in results from 2011-2012 resulting from the AlexNet submission.</span><span class="c3 c28">&nbsp;</span><span class="c3">For a review of the challenge requirements relating to Classification and Localization see: </span><span class="c21 c3">
    <a class="c4 a-word-wrap" href="http://www.image-net.org/challenges/LSVRC/2016/index%23comp">http://www.image-net.org/challenges/LSVRC/2016/index#comp</a></span><span class="c8 c3">&nbsp;</span></p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Jia Deng (2016). </span><span class="c7 c3">ILSVRC2016 object localisation: introduction, results</span><span class="c3">. Slide 2. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf">http://image-net.org/challenges/talks/2016/ILSVRC2016_10_09_clsloc.pdf</a></span><span class="c8 c3">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c12 c6">Interesting takeaways from the ImageNet LSVRC (2016):</span><span class="c6">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jklfmxwzkhun-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Scene Classification</span><span class="c6">&nbsp;refers to the task of labelling an image with a certain scene class like ‘greenhouse’, ‘stadium’, ‘cathedral’, etc. ImageNet held a Scene Classification challenge last year with a subset of the Places2</span><sup class="c6 c12"><a href="#ftnt15" id="ftnt_ref15">[15]</a></sup><span class="c6">&nbsp;dataset: 8 million images for training with 365 scene categories. <br>Hikvision</span><sup class="c6 c12"><a href="#ftnt16" id="ftnt_ref16">[16]</a></sup><span class="c5">&nbsp;won with a 9% top-5 error with an ensemble of deep Inception-style networks, and not-so-deep residuals networks. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jklfmxwzkhun-0">
    <li class="c1 c2 c13"><span class="c12 c6">Trimps-Soushen</span><span class="c6">&nbsp;won the ImageNet Classi</span><span class="c6">fication task with 2.99% top-5 classification error and 7.71% localisation error. The team employed an ensemble for classification (averaging the results of Inception, Inception-Resnet, ResNet and Wide Residual Networks models</span><sup class="c6 c12"><a href="#ftnt17" id="ftnt_ref17">[17]</a></sup><span class="c6">) and Faster R-CNN for localisation based on the labels.</span><sup class="c6 c12"><a href="#ftnt18" id="ftnt_ref18">[18]</a></sup><span class="c6">&nbsp;The dataset was distributed across 1000 image classes with 1.2 million images provided as training data. The partitioned test data compiled a further 100 thousand unseen images.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6"></span></p>
<ul class="c18 lst-kix_jklfmxwzkhun-0">
    <li class="c1 c2 c13"><span class="c12 c6">ResNeXt</span><span class="c6">&nbsp;by Facebook came a close second in top-5 classification error with 3.03% by using a new architecture that extends th</span><span class="c6">e original ResNet architecture.</span><sup class="c6 c12"><a href="#ftnt19" id="ftnt_ref19">[19]</a></sup><span class="c5">&nbsp;</span></li>
</ul>
<p class="c1 c9"><span class="c50 c12 c6 c26 c14"></span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

<h1 class="c15 main-heading" id="heading-detection"><span class="c20 c12 c25">Object Detection</span></h1>

<p class="c1"><span class="c6">As one can imagine the process of </span><span class="c12 c6">Object Detection</span><span class="c6">&nbsp;does exactly that, detects objects within images. </span><span class="c6">The definition provided for object detection by the ILSVRC 2016</span><sup class="c6 c12"><a href="#ftnt20" id="ftnt_ref20">[20]</a></sup><span class="c6 c14">&nbsp;includes outputting bounding boxes and labels for individual objects.</span><span class="c5 c14">&nbsp;This differs from the classification/localisation task by applying classification and localisation to many objects instead of just a single dominant object. </span>
</p>

<p class="c10"><span class="c20 c12 c6 c14">&nbsp;</span></p>

<p class="c10"><span class="c12 c6 c14">Figure 3</span><span class="c5 c14">: Object Detection With Face as the Only Class</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 634.50px; height: 277.00px;"><img class="border cntr" alt="" src="images/c-image8-40.jpg" style="width: 95%; max-width: 634px; /*width: 634.50px; height: 277.00px; /*margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c6 c14"><br></span><span class="c3 c12">Note: </span><span class="c3">Picture is an example of face detection, Object Detection of a single class. The authors cite one of the persistent issues in Object Detection to be the detection of small objects. Using small faces as a test class they explore the </span><span class="c8 c3 c14">role of scale invariance, image resolution, and contextual reasoning. </span>
<br><span class="c3 c12 c14">Source: </span><span class="c3 c14">Hu and Ramanan (2016, p. 1)</span><sup class="c3 c12"><a href="#ftnt21" id="ftnt_ref21">[21]</a></sup><span class="c5">&nbsp;</span></p>

<p class="c1"></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">One of 2016’s major trends in </span><span class="c12 c6">Object Detection</span><span class="c6">&nbsp;was the shift towards a quicker, more efficient detection system. This was visible in approaches like YOLO, SSD and R-FCN as a move towards sharing computation on a whole image. Hence, </span><span class="c6">differentiating</span><span class="c6">&nbsp;themselves from the costly subnetworks associated with Fast/Faster R-CNN techniques. </span><span class="c5">This is typically referred to as ‘end-to-end training/learning’
    and features throughout this piece. </span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">The rationale generally is to avoid having separate algorithms focus on their respective subproblems in isolation as this typically increase</span><span class="c6">s</span><span class="c6">&nbsp;training time and can lower network accuracy. That being said this end-to-end adaptation of networks typically takes place after initial sub-network solutions and, as such, is a retrospective optimisation. </span><span class="c6">However, Fast/Faster R-CNN techniques remain highly effective and are still used extensively for object detection.</span></p>

<p class="c1"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span></p>
<ul class="c18 lst-kix_tdd8q3xf6s8o-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">SSD: Single Shot MultiBox Detector</span><sup class="c6 c12"><a href="#ftnt22" id="ftnt_ref22">[22]</a></sup><span class="c6">&nbsp;utilises a single Neural Network which encapsulates all the necessary computation and eliminates the costly proposal generation of other methods. It achieves “</span><span class="c7 c6">75.1% mAP, outperforming a comparable state of the art Faster R-CNN model</span><span class="c5">” (Liu et al. 2016). </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_tdd8q3xf6s8o-0">
    <li class="c1 c2 c13"><span class="c6">One of the most impressive systems we saw in 2016 was from the aptly named “</span><span class="c12 c6">YOLO9000: Better, Faster, Stronger</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt23" id="ftnt_ref23">[23]</a></sup><span class="c6">, which introduces the YOLOv2 and YOLO9000 detection systems.</span><sup class="c6 c12"><a href="#ftnt24" id="ftnt_ref24">[24]</a></sup><span class="c6">&nbsp;YOLOv2 vastly improves the initial YOLO model from mid-2015,</span><sup class="c6 c12"><a href="#ftnt25" id="ftnt_ref25">[25]</a></sup><span class="c6">&nbsp;and is able to achieve better results at very high FPS </span><span class="c6">(up to 90 FPS on low resolution images using the original GTX Titan X)</span><span class="c6">. In addition to completion speed, the system outperforms Faster RCNN with ResNet and SSD on certain object detection datasets.</span></li>
</ul>
<p class="c1"><span class="c6">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p>

<p class="c1 c2"><span class="c6">YOLO9000 implements a joint training method for detection and classification extending its prediction capabilities beyond the labelled detection data available i.e. it is able to detect objects that it </span><span class="c6 c22">has never seen labelled detection data for</span><span class="c6">. The YOLO9000 model provides real-time object detection across 9000+ categories, closing the dataset size gap between classification and detection. Additional details, pre-trained models and a video showing it in action is available </span><span class="c0"><a class="c4 a-word-wrap" href="http://pjreddie.com/darknet/yolo/">here</a></span><span class="c6">.</span>
    <sup class="c6 c12"><a href="#ftnt26" id="ftnt_ref26">[26]</a><br>

</sup></p>

    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/VOC3huqHrss">
</iframe>
        </div>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_qxrwgmaqld8t-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Feature Pyramid Networks for Object Detection</span><sup class="c12 c6"><a href="#ftnt27" id="ftnt_ref27">[27]</a></sup><span class="c6">&nbsp;comes from FAIR</span>
        <sup class="c6 c12"><a href="#ftnt28" id="ftnt_ref28">[28]</a></sup><span class="c6">&nbsp;and capitalises on the “</span><span class="c7 c6">inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost</span><span class="c6">”, meaning that representations remain powerful without compromising speed or memory. Lin et al. (2016) achieve state-of-the-art (hereafter SOTA) single-model results on COCO</span><sup class="c6 c12"><a href="#ftnt29" id="ftnt_ref29">[29]</a></sup><span class="c6">. Beating the results achieved by winners in 2016 when combined with a basic Faster R-CNN system. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_eonlgzn3npwq-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">R-FCN: Object Detection via Region-based Fully Convolutional Networks</span><span class="c6">:</span><sup class="c12 c6"><a href="#ftnt30" id="ftnt_ref30">[30]</a></sup><span class="c6">&nbsp;This is another method that avoids applying a costly per-region subnetwork hundreds of times over an image by making the region-based detector fully convolutional and sharing computation on the whole image. “</span><span class="c7 c6">Our result is achieved at a test-time speed of 170ms per image, 2.5-20x faster than the Faster R-CNN counterpart</span><span class="c5">” (Dai et al., 2016).</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 4</span><span class="c6">: Accuracy tradeoffs in Object Detection </span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 639.50px; height: 364.84px;"><img alt="" class="border cntr" src="images/image16.png" style="width: 95%; max-width: 639.5px; /*width: 639.50px; height: 364.84px; /*margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c3">: Y-axis displays mAP (mean Average Precision) and the X-axis displays meta-architecture variability across each feature extractor (VGG, MobileNet...Inception ResNet V2). Additionally, mAP small, medium and large describe the average precision for small, medium and large objects, respectively. As such accuracy is “</span><span class="c7 c3">stratified by object size, meta-architecture and feature extractor</span><span class="c3">” and “</span><span class="c7 c3">image resolution is fixed to 300</span><span class="c3">”. While Faster R-CNN performs comparatively well in the above sample, it is worth noting that the meta-architecture is considerably slower than more recent approaches, such as R-FCN.</span>
</p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Huang et al. (2016, p. 9)</span><sup class="c6 c12"><a href="#ftnt31" id="ftnt_ref31">[31]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">Huang et al. (2016)</span><sup class="c6 c12"><a href="#ftnt32" id="ftnt_ref32">[32]</a></sup><span class="c6">&nbsp;present a paper which provides an in depth performance</span><span class="c28 c6">&nbsp;</span><span class="c5">comparison between R-FCN, SSD and Faster R-CNN. Due to the issues around accurate comparison of Machine Learning (ML) techniques we’d like to point to the merits of producing a standardised approach here. They view these architectures as ‘meta-architectures’
    since they can be combined with different kinds of feature extractors such as ResNet or Inception. </span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">The authors study the trade-off between accuracy and speed by varying meta-architecture, feature extractor and image resolution. The choice of feature extractor for example produces large variations between meta-architectures. </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">The trend of making object detection cheap and efficient while still retaining the accuracy required for real-time commercial applications, notably in autonomous driving applications, is also demonstrated by SqueezeDet</span><sup class="c6 c12"><a href="#ftnt33" id="ftnt_ref33">[33]</a></sup><span class="c6">&nbsp;and PVANet</span><sup class="c6 c12">
    <a href="#ftnt34" id="ftnt_ref34">[34]</a>
</sup><span class="c6">&nbsp;papers. While a Chinese company, DeepGlint, provides a good example of object detection in operation as a CCTV integration, albeit in a vaguely Orwellian manner: </span><span class="c0">
    <a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=xhp47v5OBXQ">Video</a></span><span class="c6">.</span>

    <sup class="c6 c12"><a href="#ftnt35" id="ftnt_ref35">[35]</a></sup></p>
    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/xhp47v5OBXQ">
</iframe>
        </div>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c12 c6">Results from ILSVRC and COCO Detection Challenge</span></p>

<p class="c1"><span class="c6">COCO</span><sup class="c6 c12"><a href="#ftnt36" id="ftnt_ref36">[36]</a></sup><span class="c6">&nbsp;(Common Objects in Context) is another popular image dataset. However, it is comparatively sma</span><span class="c6">ller and more curated than alternatives like ImageNet, with a focus on object recognition within the broader context of scene understanding. The organizers host a yearly challenge for Object Detection, segmentation and keypoints. Detection results from both the ILSVRC</span><sup class="c6 c12"><a href="#ftnt37" id="ftnt_ref37">[37]</a></sup><span class="c6">&nbsp;and the COCO</span><sup class="c6 c12"><a href="#ftnt38" id="ftnt_ref38">[38]</a></sup><span class="c5">&nbsp;Detection Challenge are;</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_ihpmhvl4dcp7-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">ImageNet LSVRC Object Detection from Images (DET): </span><span class="c5">CUImage 66% meanAP. Won 109 out of 200 object categories. </span></li>
    <li class="c1 c2 c13"><span class="c12 c6">ImageNet LSVRC Object Detection from video (VID): </span><span class="c5">NUIST 80.8% mean AP</span></li>
    <li class="c1 c2 c13"><span class="c12 c6">ImageNet LSVRC Object Detection from video with tracking: </span><span class="c6">CUvideo 55.8% mean AP</span></li>
    <li class="c1 c2 c13"><span class="c12 c6">COCO 2016 Detection Challenge (bounding boxes): </span><span class="c5">G-RMI (Google) 41.5% AP (4.2% absolute percentage increase from 2015 winner MSRAVC)</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">In review of the detection results for 2016, ImageNet stated that the ‘MSRAVC 2015 set a very high bar for performance [introduction of ResNets to competition]. Performance on all classes has improved across entries. Localization improved greatly in both challenges. High relative improvement on small object instances’
    (ImageNet, 2016).</span><sup class="c6 c12"><a href="#ftnt39" id="ftnt_ref39">[39]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 5</span><span class="c6">:</span><span class="c12 c6">&nbsp;</span><span class="c6">ILSVRC detection results from images (2013-2016)</span></p>

<p class="c1 c2 image-margin" style="margin-left: 0;"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 459.00px; height: 338.00px;"><img alt="" class="border cntr" src="images/image17.png" style="width: 95%; max-width: 459px; /*width: 459.00px; height: 338.00px; /*margin-left: 0.00px; margin-top: -0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c8 c3">: ILSVRC Object Detection results from images (DET) (2013-2016).</span></p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">:</span><span class="c3 c12">&nbsp;</span><span class="c3">ImageNet. 2016.</span><span class="c3 c28">&nbsp;</span><span class="c3">[Online] </span><span class="c7 c3">Workshop Presentation, Slide 2</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf</a></span><span class="c3">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-tracking"><span class="c20 c12 c25">Object Tracking</span></h1>

<p class="c1"><span class="c6">Refers to the process of following a specific object of interest, or multiple objects, in a given scene. It traditionally has applications in video and real-world interactions where observations are made following an initial </span><span class="c12 c6">object detection</span><span class="c6">; the process is crucial to autonomous driving systems for example.</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_19zzdcpw893p-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Fully-Convolutional Siamese Networks for Object Tracking</span><sup class="c12 c6"><a href="#ftnt40" id="ftnt_ref40">[40]</a></sup><span class="c5">&nbsp;combines a basic tracking algorithm with a Siamese network, trained end-to-end, which achieves SOTA and operates at frame-rates in excess of real-time. This paper attempts to tackle the lack of richness available to tracking models from traditional online learning methods.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_19zzdcpw893p-0">
    <li class="c1 c2 c13"><span class="c12 c6">Learning to Track at 100 FPS with Deep Regression Networks</span><sup class="c12 c6"><a href="#ftnt41" id="ftnt_ref41">[41]</a></sup><span class="c6">&nbsp;is another paper which attempts to ameliorate the existing issues with online training methods. The authors produce a tracker which leverages a feed-forward network to learn the generic relationships surrounding object motion, appearance and orientation which effectively track novel objects </span><span class="c6 c22">without online training</span><span class="c6">. Provides SOTA on a standard tracking benchmark while also managing “</span><span class="c7 c6">to track generic objects at 100 fps</span><span class="c5">” (Held et al., 2016). </span>
    </li>
</ul>
<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c5">Video of GOTURN (Generic Object Tracking Using Regression</span></p>

<p class="c1 c2"><span class="c6">Networks) available: </span><span class="c0"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=kMhwXnLgT_I">Video</a></span><sup class="c6 c12"><a href="#ftnt42" id="ftnt_ref42">[42]</a>
</sup>
<!--</sup>-->

    <span class="c28 c6">&nbsp;</span></p>

    <div class="embed-container">
        <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/kMhwXnLgT_I">
</iframe>
        </div>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>
<ul class="c18 lst-kix_19zzdcpw893p-0">
    <li class="c1 c2 c13"><span class="c12 c6">Deep Motion Features for Visual Tracking</span><sup class="c12 c6"><a href="#ftnt43" id="ftnt_ref43">[43]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">merge hand-crafted features, deep RGB/appearance features (from CNNs), and deep motion features (trained on optical flow images) to achieve SOTA. While deep motion features are commonplace in Action Recognition and Video Classification, the authors claim this is the first time they are used for visual tracking. The paper was also awarded </span><span class="c6 c22">Best Paper</span><span class="c5">&nbsp;in ICPR 2016, for “Computer Vision and Robot Vision” track.</span></li>
</ul>
<p class="c1 c2" style="margin-bottom: 10px;"><span class="c6"><br>“</span><span class="c7 c6">This paper presents an investigation of the impact of deep motion features in a tracking-by-detection framework. We further show that hand-crafted, deep RGB, and deep motion features contain complementary information. To the best of our knowledge, we are the first to propose fusing appearance information with deep motion features for visual tracking. Comprehensive experiments clearly suggest that our fusion approach with deep motion features outperforms standard methods relying on appearance information alone.</span><span class="c6">”<br></span></p>
<ul class="c18 lst-kix_u9pb33er4e37-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Virtual Worlds as Proxy for Multi-Object Tracking Analysis</span><sup class="c12 c6"><a href="#ftnt44" id="ftnt_ref44">[44]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">approaches the lack of true-to-life variability present in existing video-tracking benchmarks and datasets. The paper proposes a new method for real-world cloning which generates rich, virtual, synthetic, photo-realistic environments from scratch with full-labels that overcome some of the sterility present in existing datasets. The generated images are automatically labelled with accurate ground truth allowing a range of applications aside from object detection</span><span class="c6">/tracking</span><span class="c5">, such as depth and optical flow.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_u9pb33er4e37-0">
    <li class="c1 c2 c13"><span class="c12 c6">Globally Optimal Object Tracking with Fully Convolutional Networks</span><sup class="c12 c6"><a href="#ftnt45" id="ftnt_ref45">[45]</a></sup><span class="c6">&nbsp;deals with object variance and occlusion, citing these as two of the root limitations within object tracking. "</span><span class="c7 c6">Our proposed method solves the object appearance variation problem with the use of a Fully Convolutional Network and deals with occlusion by Dynamic Programming</span><span class="c5">" (Lee et al., 2016).</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-part-two"><span class="c20 c12 c25">Part Two: Segmentation, Super-res/Colourisation/Style Transfer, Action Recognition</span></h1>

<h1 class="c15 main-heading" id="heading-segmentation"><span class="c12 c25">Segmentation</span></h1>

<p class="c1"><span class="c6">Central to Computer Vision is the process of </span><span class="c6">Segmentation</span><span class="c6">, which divides whole images into pixel groupings which can then be labelled and classified. Moreover, </span><span class="c6">Semantic Segmentation </span><span class="c6">goes further by trying to semantically understand the role of each pixel in the image e.g. is it a cat, car or some other type of class?</span><span class="c6">&nbsp;</span><span class="c6">Instance Segmentation </span><span class="c5">takes this even further by segmenting different instances of classes e.g. labelling three different dogs with three different colours. It is one of a barrage of Computer Vision applications currently employed in autonomous driving technology suites.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">Perhaps some of the best improvements in the area of segmentation come courtesy of FAIR, who continue to build upon their DeepMask work from 2015.</span><sup class="c6 c12">
    <a href="#ftnt46" id="ftnt_ref46">[46]</a></sup><span class="c6">&nbsp;DeepMask generates rough ‘masks’
    over objects as an initial form of segmentation. In 2016, Fair introduced SharpMask</span><sup class="c6 c12"><a href="#ftnt47" id="ftnt_ref47">[47]</a></sup><span class="c6">&nbsp;which refines the ‘masks’ provided by DeepMask, correcting the loss of detail and improving semantic segmentation</span><span class="c6">.</span><span class="c6">&nbsp;In addition to this, MultiPathNet</span><sup class="c6 c12"><a href="#ftnt48" id="ftnt_ref48">[48]</a></sup><span class="c5">&nbsp;identifies the objects delineated by each mask. </span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">“</span><span class="c7 c6">To capture general object shape, you have to have a high-level understanding of what you are looking at (DeepMask), but to accurately place the boundaries you need to look back at lower-level features all the way down to the pixels (SharpMask).</span><span class="c6">”</span><span class="c6">&nbsp;- Piotr Dollar, 2016.</span><sup class="c6 c12"><a href="#ftnt49" id="ftnt_ref49">[49]</a></sup><span class="c5">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 6</span><span class="c5">: Demonstration of FAIR techniques in action</span></p>

<p class="c10 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 541.00px; height: 361.00px;"><img alt="" class="border cntr" src="images/c-image11.jpg" style="width: 100%; max-width: 541px; /*width: 541.00px; height: 361.00px;/* margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c8 c3">: The above pictures demonstrate the segmentation techniques employed by FAIR. These include the application of DeepMask, SharpMask and MultiPathNet techniques which are applied in that order. This process allows accurate segmentation and classification in a variety of scenes. </span>
</p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Dollar (2016).</span><sup class="c6 c12"><a href="#ftnt50" id="ftnt_ref50">[50]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c12 c6">Video Propagation Networks</span><sup class="c12 c6"><a href="#ftnt51" id="ftnt_ref51">[51]</a></sup><span class="c6">&nbsp;attempt to create a simple model to propagate accurate object masks, assigned at first frame, through the entire video sequence along with some additional information. </span>
</p>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>

<p class="c1"><span class="c6">In 2016, researchers worked on </span><span class="c6">finding alternative network configurations to tackle the </span><span class="c6">aforementioned issues of scale and localisation. DeepLab</span><sup class="c6"><a href="#ftnt52" id="ftnt_ref52">[52]</a></sup><span class="c6">&nbsp;is one such example of this which achieves encouraging results for semantic image segmentation tasks. Khoreva et al. (</span><span class="c6">2016</span><span class="c6">)</span><sup class="c6 c12"><a href="#ftnt53" id="ftnt_ref53">[53]</a></sup><span class="c6">&nbsp;build on Deeplab’s earlier work (circa 2015)</span><span class="c28 c6">&nbsp;</span><span class="c6">and propose a weakly supervised training method which achieves comparable results to fully supervised networks. </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">Computer Vision further refined the network sharing of useful information approach through the use of end-to-end networks, which reduce the computational requirements of multiple omni-directional subtasks for classification. Two key papers using this approach are:</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_3eigggtxldag-0 start">
    <li class="c1 c2 c13" style="margin-bottom:10px;"><span class="c12 c6">100 Layers Tiramisu</span><sup class="c12 c6"><a href="#ftnt54" id="ftnt_ref54">[54]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">is a fully-convolutional DenseNet which</span><span class="c6">&nbsp;connects every layer, </span><span class="c6">to every other layer, </span><span class="c6">in a feed-forward fashion. It also </span><span class="c6">achieves SOTA on multiple benchmark datasets with fewer parameters and training/processing. </span>
    </li>
</ul>
<ul class="c18 lst-kix_nk9139oynrts-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Fully Convolutional Instance-aware Semantic Segmentation</span><sup class="c12 c6"><a href="#ftnt55" id="ftnt_ref55">[55]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">performs instance mask prediction and classification jointly (two subtasks). <br></span><span class="c12 c6">COCO Segmentation challenge winner </span><span class="c6">MSRA. 37.3% AP. <br>9.1% absolute jump from MSRAVC in 2015 in COCO challenge. </span></li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6"></span></p>

<p class="c1"><span class="c6">While </span><span class="c12 c6">ENet</span><span class="c6">,</span><sup class="c6 c12"><a href="#ftnt56" id="ftnt_ref56">[56]</a></sup><span class="c6">&nbsp;</span><span class="c6">a DNN architecture for real-time semantic segmentation, is not of this category, it does demonstrate the commercial merits of reducing computation costs and giving greater access to mobile devices.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">Our work wishes to relate as much of these advancements back to tangible public applications as possible. With this in mind, the following contains some of the most interesting </span><span class="c12 c6">healthcare </span><span class="c12 c6">application</span><span class="c5">&nbsp;of segmentation in 2016;</span></p>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>
<ul class="c18 lst-kix_vu11v61n8lsf-0 start" style="margin-bottom: 15px;">
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.00799">A Benchmark for Endoluminal Scene Segmentation
        of Colonoscopy Images</a></span><sup class="c6 c12"><a href="#ftnt57" id="ftnt_ref57">[57]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.03925v1">3D fully convolutional networks for
        subcortical segmentation in MRI: A large-scale study</a></span><sup class="c6 c12"><a href="#ftnt58" id="ftnt_ref58">[58]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.08664v3">Semi-supervised Learning using Denoising
        Autoencoders for Brain Lesion Detection and Segmentation</a></span><sup class="c6 c12"><a href="#ftnt59" id="ftnt_ref59">[59]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.09811">3D Ultrasound image segmentation: A Survey</a></span><sup class="c6 c12"><a href="#ftnt60" id="ftnt_ref60">[60]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.02064">A Fully Convolutional Neural Network based
        Structured Prediction Approach Towards the Retinal Vessel Segmentation</a></span><sup class="c6 c12"><a href="#ftnt61" id="ftnt_ref61">[61]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.04534">3-D Convolutional Neural Networks for
        Glioblastoma Segmentation</a></span><sup class="c6 c12"><a href="#ftnt62" id="ftnt_ref62">[62]</a></sup><span class="c0"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.04534"><br></a></span>
    </li>
</ul>
<p class="c1"><span class="c6">One of our favourite quasi-medical segmentation applications is </span><span class="c12 c6">FusionNet</span><sup class="c12 c6"><a href="#ftnt63" id="ftnt_ref63">[63]</a></sup><span class="c6">- a deep fully residual convolutional neural network for image segmentation in connectomics</span><sup class="c6 c12"><a href="#ftnt64" id="ftnt_ref64">[64]</a></sup><span class="c6">&nbsp;benchmarked against SOTA electron microscopy (EM) segmentation methods.</span></p>
    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/PNzQ4PNZSzc">
</iframe>
        </div>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<h1 class="c15 main-heading" id="heading-super-res"><span class="c12 c25">Super-resolution, Style Transfer &amp; </span><span class="c25">Colourisation </span></h1>

<p class="c1"><span class="c6">Not all research in Computer Vision serves to extend the pseudo-cognitive abilities of machines, and often the fabled malleability of neural networks, as well as other ML techniques, lend themselves to a variety of other novel applications that spill into the public space. Last year’s advancements in Super-resolution, </span><span class="c6">Style Transfer &amp; Colourisation</span><span class="c28 c6">&nbsp;</span><span class="c5">occupied that space for us. <br></span></p>
<br>
<p class="c1"><span class="c12 c6">Super-resolution </span><span class="c6">refers to the process of estimating a high resolution image from a low resolution counterpart, and also the prediction of image features at different magnifications, something which the human brain can do almost effortlessly. </span><span class="c6">Originally super-resolution was performed by simple techniques like bicubic-interpolation and nearest neighbours.</span><span class="c5">&nbsp;In terms of commercial applications, the desire to overcome low-resolution constraints stemming from source quality and realisation of ‘CSI Miami’
    style image enhancement has driven research in the field. Here are some of the year’s advances and their potential impact:</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_xj6qqr6gdbue-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Neural Enhance</span><sup class="c12 c6"><a href="#ftnt65" id="ftnt_ref65">[65]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">is the brainchild of </span><span class="c5 c14">Alex J. Champandard and combines approaches from four different research papers to achieve its Super-resolution method. </span>
    </li>
    <li class="c1 c2 c13"><span class="c12 c6 c14">Real-Time Video Super Resolution </span><span class="c6 c14">was also attempted in 2016 in two notable instances.</span><sup class="c12 c6 c14"><a href="#ftnt66" id="ftnt_ref66">[66]</a></sup><span class="c12 c6 c45 c14">,</span><sup class="c12 c6 c14"><a href="#ftnt67" id="ftnt_ref67">[67]</a></sup></li>
    <li class="c1 c2 c13"><span class="c12 c6 c14">RAISR: </span><span class="c6 c14">Rapid and Accurate Image Super-Resolution</span><sup class="c6 c12"><a href="#ftnt68" id="ftnt_ref68">[68]</a></sup><span class="c6 c14">&nbsp;from Google avoids the costly memory and speed requirements of neural network approaches by training filters with low-resolution and high-resolution image pairs. RAISR, as a learning-based framework, is two orders of magnitude faster than competing algorithms and has minimal memory requirements when </span><span class="c6 c14">compared with neural network-based approaches</span><span class="c6 c14">. Hence super-resolution is extendable to personal devices. There is a research blog available </span><span class="c0 c14"><a class="c4" href="https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html">here</a></span><span class="c6 c14">.</span><sup class="c6 c12"><a href="#ftnt69" id="ftnt_ref69">[69]</a></sup><span class="c5 c14">&nbsp;</span></li>
</ul>
<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c10"><span class="c12 c6 c14">Figure 7</span><span class="c6 c14">: Super-resolution SRGAN examp</span><span class="c5 c14">le</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 244.40px;"><img alt="" class="border cntr" src="images/c-image10.jpg" style="width: 100%; max-width: 624px; /*width: 624.00px; height: 307.59px; /*margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3 c12">Note</span><span class="c3">: From left to right: bicubic interpolation (the objective worst performer for focus), Deep residual network optimised for MSE, deep residual generative adversarial network optimized for a loss more sensitive to human perception, original High Resolution (HR) image. Corresponding peak signal to noise ratio (PSNR) and structural similarity (SSIM) are shown in two brackets. [4 x upscaling] The reader may wish to zoom in on the middle two images (SRResNet and SRGAN) to see the difference between image smoothness vs more realistic fine details.<br></span><span class="c3 c12">Source</span><span class="c3">: Ledig et al. (2017)</span><sup class="c3"><a href="#ftnt70" id="ftnt_ref70">[70]</a></sup></p>

<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c1"><span class="c6 c14">The use of Generative Adversarial Networks (GANs) represent current SOTA for Super-resolution:</span></p>
<ul class="c18 lst-kix_3a0j39gr05nl-0 start">
    <li class="c1 c2 c13"><span class="c12 c6 c14">SRGAN</span><sup class="c12 c6 c14"><a href="#ftnt71" id="ftnt_ref71">[71]</a></sup><span class="c12 c6 c14">&nbsp;</span><span class="c5 c14">provides photo-realistic textures from heavily downsampled images on public benchmarks, using a discriminator network trained to differentiate between super-resolved and original photo-realistic images. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c1 c2"><span class="c6 c14">Qualitatively SRGAN performs the best, although SRResNet performs best with peak-signal-to-noise-ratio (PSNR) metric but SRGAN gets the finer texture details and achieves the best Mean Opinion Score (MOS). </span><span class="c7 c6 c14">“To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4× upscaling factors.”</span><sup class="c7 c12"><a href="#ftnt72" id="ftnt_ref72">[72]</a></sup><span class="c28 c6 c14">&nbsp;</span><span class="c5 c14">All previous approaches fail to recover the finer texture details at large upscaling factors. </span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_s5md9ry0mo50-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Amortised MAP Inference for Image Super-resolution</span><sup class="c12 c6"><a href="#ftnt73" id="ftnt_ref73">[73]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">proposes a method for calculation of Maximum a Posteriori (MAP) inference using a Convolutional Neural Network. However, their research presents three approaches for optimisation, all of which GANs perform markedly better </span><span class="c6">on real image data at present.</span><span class="c6">&nbsp;</span></li>
</ul>

<p class="c10" style="margin-top: 10px;"><span class="c12 c6 c14">Figure 8</span><span class="c6 c14">: Style Transfer from Nikulin &amp; Novak</span><span class="c5 c14">le</span></p>

<p class="c47">
    <!--<span><span class="c12 c6">Figure 8</span><span class="c6">: Style Transfer from Nikulin &amp; Novak</span></span>-->
    <span class="image-margin" style="/*overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 431.00px; height: 300.00px;">
        <img alt="" class="border cntr" src="images/c-image20.jpg" style="width: 80%; max-width: 440px; /*width: 440.00px; height: 425.96px; margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c3"><br></span><span class="c3 c12">Note</span><span class="c3">: Transferring different styles to a photo of a cat (original top left).<br></span><span class="c3 c12">Source</span><span class="c3">: Nikulin &amp; Novak (2016)</span>
</p>

<p class="c46"><span class="c6">Undoubtedly, </span><span class="c12 c6">Style Transfer</span><span class="c6">&nbsp;epitomises a novel use of neural networks that has ebbed into the public domain, specifically through last year’s facebook integrations and companies like </span><span class="c6">Prisma</span><sup class="c6 c12"><a href="#ftnt74" id="ftnt_ref74">[74]</a></sup><span class="c6">&nbsp;and Artomatix</span><sup class="c6 c12"><a href="#ftnt75" id="ftnt_ref75">[75]</a></sup><span class="c6">. Style transfer is an older technique but converted to a neural networks in 2015 with the publication of </span><span class="c6">a</span><span class="c6">&nbsp;Neural Algorithm of Artistic Style.</span><sup class="c12 c6"><a href="#ftnt76" id="ftnt_ref76">[76]</a></sup><span class="c6">&nbsp;Since then, the concept of style transfer was expanded upon by Nikulin and Novak</span><sup class="c6 c12"><a href="#ftnt77" id="ftnt_ref77">[77]</a></sup><span class="c6">&nbsp;and also applied to video,</span><sup class="c6 c12"><a href="#ftnt78" id="ftnt_ref78">[78]</a></sup><span class="c5">&nbsp;as is the common progression within Computer Vision. </span>
</p>

<p class="c47" style="padding-bottom: 0px;"><span class="c12 c6">Figure 9</span><span class="c6">: Further examples of Style Transfer</span></p>

<p class="c46"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 548.00px; height: 328.60px;"><img alt="" class="border cntr" src="images/c-image18.jpg" style="width: 100%; max-width: 548px; /*width: 548.00px; height: 366.71px; /*margin-left: 0.00px; margin-top: 5.13px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c6"><br></span><span class="c3 c12">Note</span><span class="c3">: The top row (left to right) represent the artistic style which is transposed onto the original images which are displayed in the first column (Woman, Golden Gate Bridge and Meadow Environment). Using conditional instance normalisation a single style transfer network can capture 32 style simultaneously, five of which are displayed here. The full suite of images in available in the source paper’s appendix. This work will feature in the International Conference on Learning Representations (ICLR) 2017. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;<br></span><span class="c3 c12">Source</span><span class="c3">: Dumoulin et al. (2017, p. 2)</span><sup class="c6 c12"><a href="#ftnt79" id="ftnt_ref79">[79]</a></sup><span class="c3"><br><br></span><span class="c6">Style transfer as a topic is fairly intuitive once visualised; take an image and imagine it with the stylistic features of a different image. For example, in the style of a famous painting or artist. This year Facebook released Caffe2Go,</span><sup class="c6 c12"><a href="#ftnt80" id="ftnt_ref80">[80]</a></sup><span class="c6">&nbsp;their deep learning system which integrates into mobile devices. Google also released some interesting work which sought to blend multiple styles to generate entirely unique image styles: Research blog</span><sup class="c6 c12"><a href="#ftnt81" id="ftnt_ref81">[81]</a></sup><span class="c6">&nbsp;and full paper.</span><sup class="c6 c12"><a href="#ftnt82" id="ftnt_ref82">[82]</a></sup><span class="c5">&nbsp;<br><br>Besides mobile integrations, style transfer has applications in the creation of game assets. Members of our team recently saw a presentation by the Founder and CTO of Artomatix, Eric Risser, who discussed the technique’s novel application for content generation in games (texture mutation, etc.) and, therefore, dramatically minimises the work of a conventional texture artist. </span>
</p>

<p class="c1"><span class="c12 c6">Colourisation</span><span class="c6">&nbsp;</span><span class="c5">is the process of changing monochrome images to new full-colour versions. Originally this was done manually by people who painstakingly selected colours to represent specific pixels in each image. In 2016, it became possible to automate this process while maintaining the appearance of realism indicative of the human-centric colourisation process. While humans may not accurately represent the true colours of a given scene, their real world knowledge allows the application of colours in a way which is consistent with the image and another person viewing said image.<br><br>The process of colourisation is interesting in that the network assigns the most likely colouring for images based on its understanding of object location, textures and environment, e.g. it learns that skin is pinkish and the sky is blueish.<br><br>Three of the most influential works of the year are as follows:</span>
</p>
<ul class="c18 lst-kix_8j5v1moclfjn-0 start">
    <li class="c1 c2 c13"><span class="c6">Zhang et al.</span><sup class="c6 c12"><a href="#ftnt83" id="ftnt_ref83">[83]</a></sup><span class="c5">&nbsp;produced a method that was able to successfully fool humans on 32% of their trials. Their methodology is comparable to a “colourisation Turing test.”</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_8j5v1moclfjn-0">
    <li class="c1 c2 c13"><span class="c6">Larsson et al.</span><sup class="c6 c12"><a href="#ftnt84" id="ftnt_ref84">[84]</a></sup><span class="c5">&nbsp;fully automate their image colourisation system using Deep Learning for Histogram estimation.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_8j5v1moclfjn-0">
    <li class="c1 c2 c13"><span class="c6">Finally, Lizuka, Simo-Serra and Ishikawa</span><sup class="c6 c12"><a href="#ftnt85" id="ftnt_ref85">[85]</a></sup><span class="c6">&nbsp;demonstrate a colourisation model also based upon CNNs. The work outperformed the existing SOTA, we [the team] feel as though this work is qualitatively best also, appearing to be the most realistic. Figure 10 provides comparisons, however the image is taken from Lizuka et al.<br></span>
    </li>
</ul>
<p class="c10" style="margin-top: 10px;"><span class="c12 c6 c14">Figure 10</span><span class="c6 c14">: Comparison of Colourisation Research</span><span class="c5 c14">le</span></p>

<!--<p class="c47"><span class="c12 c6">Figure 10</span><span class="c5">: Comparison of Colourisation Research</span></p>-->

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 503.40px;"><img alt="" class="border cntr" src="images/c-image7.jpg" style="width:90%; max-width: 624px; /*width: 624.00px; height: 632.00px; /*margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c3">: From top to bottom - &nbsp;column one contains the original monochrome image input which is subsequently colourised through various techniques. The remaining columns display the results generated by other prominent colourisation research in 2016. When viewed from left to right, these are Larsson et al.</span>
    <span class="c3 c45">84</span>
    <span class="c3">&nbsp;2016 (column two), Zhang et al.</span>
    <span class="c3 c45">83</span><span class="c3">&nbsp;2016 (Column three), and Lizuka, Simo-Serra and Ishikawa.</span>
    <span class="c3 c45">85</span>
    <!--<sup class="c6 c12"><a href="#ftnt86" id="ftnt_ref86">[86]</a></sup>-->
    <span class="c8 c3">&nbsp;2016, also referred to as “ours” by the authors (Column four). The quality difference in colourisation is most evident in row three (from the top) which depicts a group of young boys. We believe Lizuka et al.’s work to be qualitatively superior (Column four).</span>
</p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Lizuka et al. 2016</span><sup class="c6 c12"><a href="#ftnt86" id="ftnt_ref86">[86]</a></sup></p>

<p class="c1 c2"><span class="c6"><br>“</span><span class="c7 c6">Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN.</span><span class="c6">” <br><br>In a test to see how natural their colourisation was, users were given a random image from their models and were asked, "does this image look natural to you?"<br><br>Their approach achieved 92.6%, the baseline achieved roughly 70% and the ground truth (the actual colour photos) were considered 97.7% of the time to be natural.</span>
</p>

<h1 class="c44 main-heading" id="heading-action-recogntion"><span class="c25"><br><br></span><span class="c20 c12 c25">Action Recognition</span></h1>

<p class="c1"><span class="c5">The task of action recognition refers to the both the classification of an action within a given video frame, and more recently, algorithms which can predict the likely outcomes of interactions given only a few frames before the action takes place. In this respect we see recent research attempt to imbed context into algorithmic decisions, similar to other areas of Computer Vision. Some key papers in this space are: </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_w7w41dfpoe57-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Long-term Temporal Convolutions for Action Recognition</span><sup class="c6 c12"><a href="#ftnt87" id="ftnt_ref87">[87]</a></sup><span class="c6">&nbsp;leverages the spatio-temporal structure of human actions, i.e. the particular movement and duration, to correctly recognise actions using a CNN variant. To overcome the sub-optimal temporal modelling of longer term actions by CNNs, the authors propose a neural network with long-term temporal convolutions (LTC-CNN) to improve the accuracy of action recognition. Put simply,</span><span class="c5">&nbsp;the LTCs can look at larger parts of the video to recognise actions. Their approach uses and extends 3D CNNs ‘to enable action representation at a fuller temporal scale’. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">We report state-of-the-art results on two challenging benchmarks for human action recognition UCF101 (92.7%) and HMDB51 (67.2%).</span><span class="c5">” </span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_w7w41dfpoe57-0">
    <li class="c1 c2 c13"><span class="c12 c6">Spatiotemporal Residual Networks for Video Action Recognition</span><sup class="c12 c6"><a href="#ftnt88" id="ftnt_ref88">[88]</a></sup><span class="c5">&nbsp;apply a variation of two stream CNN to the task of action recognition, which combines techniques from both traditional CNN approaches and recently popularised Residual Networks (ResNets). The two stream approach takes its inspiration from a neuroscientific hypothesis on the functioning of the visual cortex, i.e. separate pathways recognise object shape/colour and movement. The authors combine the classification benefits of ResNets by injecting residual connections between the two CNN streams.</span>
    </li>
</ul>
<p class="c1 c2"><span class="c6"><br>“</span><span class="c7 c6">Each stream initially performs video recognition on its own and for final classification, softmax scores are combined by late fusion. To date, this approach is the most effective approach of applying deep learning to action recognition, especially with limited training data. In our work we directly convert image ConvNets into 3D architectures and show greatly improved performance over the two-stream baseline.</span><span class="c5">” - 94% on UCF101 and 70.6% on HMDB51. Feichtenhofer et al. made improvements over traditional improved dense trajectory (iDT) methods and generated better results through use of both techniques.</span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_w7w41dfpoe57-0">
    <li class="c1 c2 c13"><span class="c12 c6">Anticipating Visual Representations from Unlabeled Video</span><sup class="c6 c12"><a href="#ftnt89" id="ftnt_ref89">[89]</a></sup><span class="c6">&nbsp;is an interesting paper, although not strictly action classification. The program </span><span class="c6 c22">predicts the action</span><span class="c6">&nbsp;which is likely to take place given a sequence of video frames up to one second before an action. The approach uses visual representations rather than pixel-by-pixel classification, which means that the program can operate without labeled data, by taking advantage of the feature learning properties of deep neural networks.</span><sup class="c6 c12"><a href="#ftnt90" id="ftnt_ref90">[90]</a></sup></li>
</ul>
<p class="c1 c2"><span class="c6"><br>"</span><span class="c7 c6">The key idea behind our approach is that we can train deep networks to predict the visual representation of images in the future. Visual representations are a promising prediction target because they encode images at a higher semantic level than pixels yet are automatic to compute. We then apply recognition algorithms on our predicted representation to anticipate objects and actions</span><span class="c5">".<br></span></p>
<ul class="c18 lst-kix_w7w41dfpoe57-0" style="margin-top: 12px;">
    <li class="c1 c2 c13"><span class="c6">The organisers of the </span><span class="c12 c6">Thumos Action Recognition Challenge</span><sup class="c6 c12"><a href="#ftnt91" id="ftnt_ref91">[91]</a></sup><span class="c5">&nbsp;released a paper describing the general approaches for Action Recognition from the last number of years. The paper also provides a rundown of the Challenges from 2013-2015, future directions for the challenge and ideas on how to give computers a more holistic understanding of video through Action Recognition. We hope that the Thumos Action Recognition Challenge returns in 2017 after its (seemingly) unexpected hiatus. </span>
    </li>
</ul>

    <h1 class="c15 main-heading" id="heading-part-three"><span class="c25">Part Three: </span><span class="c20 c12 c25">Toward a 3D understanding of the world</span></h1>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c6">“</span><span class="c7 c6">A key goal of Computer Vision is to recover the underlying 3D structure from 2D observations of the world.</span><span class="c6">” - Rezende et al. (2016, p. 1)</span><sup class="c6 c12"><a href="#ftnt92" id="ftnt_ref92">[92]</a></sup></p>

<p class="c1"><span class="c6"><br>In Computer Vision, the classification of scenes, objects and activities, along with the output of bounding boxes and image segmentation is, as we have seen, the focus of much new research. In essence, these approaches apply computation to gain an ‘understanding’
    of the 2D space of an image. However, detractors note that a 3D understanding is imperative for systems to successfully </span><span class="c6">interpret, and navigate, the real world.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">For instance, a network may locate a cat in an image, colour all of its pixels and classify it as a cat. But does the network </span><span class="c6">fully</span><span class="c28 c6">&nbsp;</span><span class="c6">understand where the cat in the image is, in the context of the cat’s environment? </span>
</p>

<p class="c1"><span class="c6"><br>One could argue that the computer learns very little about the 3D world from the above tasks. Contrary to this, humans understand the world in 3D even when examining 2D pictures, i.e. perspective, occlusion, depth, how objects in a scene are related</span><span class="c6">,</span><span class="c28 c6">&nbsp;</span><span class="c6">etc. Imparting these 3D representations and their associated knowledge to artificial systems represents one of the next great frontiers of Computer Vision. A major reason for thinking this is that</span><span class="c6">, generally;</span><span class="c5">&nbsp;</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">“</span><span class="c7 c6">the 2D projection of a scene is a complex function of the attributes and positions of the camera, lights and objects that make up the scene. If endowed with 3D understanding, agents can abstract away from this complexity to form stable, disentangled representations, e.g., recognizing that a chair is a chair whether seen from above or from the side, under different lighting conditions, or under partial occlusion.</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt93" id="ftnt_ref93">[93]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">However, 3D understanding has traditionally faced several impediments. The first concerns the problem of both ‘</span><span class="c6">self and normal occlusion’</span><span class="c6">&nbsp;along with the numerous 3D shapes which fit a given 2D representation. Understanding problems are further compounded by the inability to map different images of the same structures to </span><span class="c6">the same </span><span class="c6">3D space, and in the handling of the multi-modality of these representations.</span><sup class="c6 c12"><a href="#ftnt94" id="ftnt_ref94">[94]</a></sup><span class="c6">&nbsp;Finally, ground-truth 3D datasets were traditionally quite expensive and difficult to obtain which, when coupled with divergent approaches for representing 3D structures, may have led to training limitations</span><span class="c6">.</span></p>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>

<p class="c1"><span class="c6">We feel that the</span><span class="c6">&nbsp;work being conducted in this space is important to be mindful of. From the embryonic, albeit titillating early theoretical applications for future AGI systems and robotics, to the immersive, captivating applications in augmented, virtual and mixed reality which will affect our societies in the near future. We cautiously predict exponential growth in this area of Computer Vision, as a result of lucrative commercial applications, which means that soon computers may </span><span class="c6">start reasoning about the </span><span class="c6 c22">world</span><span class="c6">&nbsp;rather than just about </span><span class="c6 c22">pixels</span><span class="c6">.</span><span class="c5"><br></span></p>

<p class="c1 c9"><span class="c5"></span></p>

    <h1 class="c15 main-heading" id="heading-3d-objects"><span class="c12">3D Objects</span></h1>
<p class="c1"><span class="c12 c6"></span></p>

<p class="c1"><span class="c6">This first section is a tad scattered, acting as a catch-all for computation applied to objects represented with 3D data, inference of 3D object shape from 2D images and Pose Estimation; determining the transformation of an object’s 3D pose from 2D images.</span><sup class="c6 c12"><a href="#ftnt95" id="ftnt_ref95">[95]</a></sup><span class="c6">&nbsp;The process of reconstruction also creeps in ahead of the following section which deals with it explicitly.</span><span class="c28 c6">&nbsp;</span><span class="c5">However, with these points in mind, we present the work which excited our team the most in this general area:</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_cltoahc9dmiq-0 start">
    <li class="c1 c2 c13"><span class="c12 c6 c14">OctNet: Learning Deep 3D Representations at High Resolutions</span><sup class="c12 c6 c14"><a href="#ftnt96" id="ftnt_ref96">[96]</a></sup><span class="c6 c14">&nbsp;continue</span><span class="c6 c14">s</span><span class="c6 c14">&nbsp;the recent development of convolutional networks which operate on 3D data, or Voxels (which are like 3D pixels), using 3D convolutions. OctNet is ‘</span><span class="c6 c14">a novel 3D representation which makes deep learning with high-resolution inputs tractable</span><span class="c6 c14">’. The authors test OctNet representations by ‘analyzing the impact of resolution on several 3D tasks including 3D object classification, orientation estimation and point cloud labeling.’ The paper’s central contribution is its exploitation of sparsity </span><span class="c5 c14">in 3D input data which then enables much more efficient use of memory and computation.</span></li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6 c14"></span></p>
<ul class="c18 lst-kix_7rbz49bxi8wy-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">ObjectNet3D: A Large Scale Database for 3D Object Recognition</span><sup class="c6 c12"><a href="#ftnt97" id="ftnt_ref97">[97]</a></sup><span class="c6">&nbsp;- contributes a database for 3D object recognition, presenting 2D images and 3D shapes for 100 object categories. ‘</span><span class="c7 c6">Objects in the images in our database [taken from ImageNet] are aligned with the 3D shapes [taken from the ShapeNet repository], and the alignment provides both accurate 3D pose annotation and the closest 3D shape annotation for each 2D object.</span><span class="c6">’ Baseline experiments are provided on</span><span class="c6">: Region proposal generation, 2D object detection, joint 2D detection and 3D object pose estimation, and image-based 3D shape retrieval.</span><span class="c5"><br></span><br></li>
    <li class="c1 c2 c13"><span class="c12 c6">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction</span><sup class="c12 c6"><a href="#ftnt98" id="ftnt_ref98">[98]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">- creates a reconstruction of an object ‘in the form of a 3D occupancy grid using single or multiple images of object instance from arbitrary viewpoints.’ Mappings from images of objects to 3D shapes are learned using primarily synthetic data, and the network can train and test without requiring ‘any image annotations or object class labels’. The network comprises a 2D-CNN, a 3D Convolutional LSTM (an architecture newly created for purpose) and a 3D Deconvolutional Neural Network. How these different components interact </span><span class="c6">and are trained together end-to-end</span><span class="c28 c6">&nbsp;</span><span class="c5">is a perfect illustration of the layering capable with Neural Networks.</span></li>
</ul>
<p class="c10 c2"><span class="c6"><br></span><span class="c12 c6">Figure 11</span><span class="c5">: Example of 3D-R2N2 functionality</span></p>

<p class="c10 image-margin"><span style="/*overflow: hidden; display: inline-block; */margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 214.67px;"><img alt="Screenshot 2017-03-07 18.08.04.png" src="images/c-image4.jpg" class="border cntr" style="width: 90%; max-width: 624px; /*width: 624.00px; height: 214.67px; margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c8 c3">: Images taken from Ebay (left) and an overview of the functionality of 3D-R2N2 (right). </span></p>

<p class="c1"><span class="c3 c12">Note from source</span><span class="c8 c3">: Some sample images of the objects we [the authors] wish to reconstruct - notice that views are separated by a large baseline and objects’ appearance shows little texture and/or are non-lambertian. (b) An overview of our proposed 3D-R2N2: The network takes a sequence of images (or just one image) from arbitrary (uncalibrated) viewpoints as input (in this example, 3 views of the armchair) and generates voxelized 3D reconstruction as an output. The reconstruction is incrementally refined as the network sees more views of the object.</span>
</p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Choy et al. (2016, p. 3)</span><sup class="c6 c12"><a href="#ftnt99" id="ftnt_ref99">[99]</a></sup></p>

<p class="c1 c9"><span class="c8 c3"></span></p>

<p class="c1 c2"><span class="c5">3D-R2N2 generates ‘rendered images and voxelized models’ using ShapeNet models and facilitates 3D object reconstruction where structure from motion (SfM) and simultaneous localisation and mapping (SLAM) approaches typically fail: &nbsp;</span>
</p>

<p class="c1 c2"><span class="c6"><br>“</span><span class="c7 c6">Our extensive experimental analysis shows that our reconstruction framework i) outperforms the state-of-the-art methods for single view reconstruction, and ii) enables the 3D reconstruction of objects in situations when traditional SFM/SLAM methods fail.</span><span class="c5">”</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_7rbz49bxi8wy-0">
    <li class="c1 c2 c13"><span class="c12 c6">3D Shape Induction from 2D Views of Multiple Objects</span><sup class="c12 c6"><a href="#ftnt100" id="ftnt_ref100">[100]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">uses “</span><span class="c7 c6">Projective Generative Adversarial Networks</span><span class="c5">” (PrGANs), which train a deep generative model allowing accurate representation of 3D shapes, with the discriminator only being shown 2D images. The projection module captures the 3D representations and converts them to 2D images before passing to the discriminator. Through iterative training cycles the generator improves projections by improving the 3D voxel shapes it generates. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 12</span><span class="c5">: PrGAN architecture segment</span></p>

<p class="c1"><span class="c5">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p>

<p class="c1"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 196.00px;"><img alt="" class="border cntr" src="images/c-image1.jpg" style="width: 90%; max-width: 624px; /*width: 624.00px; height: 196.00px; margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1 c2"><span class="c3 c12">Note </span><span class="c3 c12">from source</span><span class="c3">: The PrGAN architecture for generating 2D images of shapes. A 3D voxel representation (32</span><span class="c3 c45">3</span><span class="c3">) and viewpoint are independently generated from the input z (201-d vector). The projection module renders the voxel shape from a given viewpoint (θ, φ) to create an image. The discriminator consists of 2D convolutional and pooling layers and aims to classify if the input image is generated or real. </span><span class="c6"><br></span><span class="c3 c12">Source</span><span class="c3">: Gadhelha et al. (2016, p. 3)</span><sup class="c6 c12"><a href="#ftnt101" id="ftnt_ref101">[101]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c5">In this way the inference ability is learned through an unsupervised environment: </span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">The addition of a projection module allows us to infer the underlying 3D shape distribution without using any 3D, viewpoint information, or annotation during the learning phase. </span><span class="c5">” </span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c5">Additionally, the internal representation of the shapes can be interpolated, meaning discrete commonalities in voxel shapes allow transformations from object to object, e.g. from car to aeroplane.</span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_4wkftt7qogfl-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Unsupervised Learning of 3D Structure from Images</span><sup class="c12 c6"><a href="#ftnt102" id="ftnt_ref102">[102]</a></sup><span class="c6">&nbsp;presents a completely unsupervised, generative model which demonstrates ‘the feasibility of learning to infer 3D representations of the world’
        for the first time. In a nutshell the DeepMind team present a model which “</span><span class="c7 c6">learns strong deep generative models of 3D structures, and recovers these structures from 3D and 2D images via probabilistic inference</span><span class="c6">”,</span><span class="c6">&nbsp;meaning that inputs can be both 3D and 2D.<br></span>
    </li>
</ul>
<p class="c1 c2"><span class="c6">DeepMind’s strong generative model runs on both volumetric and mesh-based representations. The use of Mesh-based representations with OpenGL allows more knowledge to be built in, e.g. how light affects the scene and the materials used. “</span><span class="c7 c6">Using a 3D mesh-based representation and training with a fully-fledged black-box renderer in the loop enables learning of the interactions between an object’s colours, materials and textures, positions of lights, and of other objects</span><span class="c6">.”</span><sup class="c6 c12"><a href="#ftnt103" id="ftnt_ref103">[103]</a></sup><span class="c5"><br><br>The models are of high quality, capture uncertainty and are amenable to probabilistic inference, allowing for applications in 3D generation and simulation. The team achieve the first quantitative benchmark for 3D density modelling on 3D MNIST and ShapeNet. This approach demonstrates that models may be trained end-to-end unsupervised on 2D images, requiring no ground-truth 3D labels.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-human-pose"><span class="c12">Human Pose Estimation and Keypoint Detection</span></h1>

<p class="c1"><span class="c6">Human Pose Estimation attempts to find the orientation and configuration of human body parts. 2D Human Pose Estimation, or Keypoint Detection, generally re</span><span class="c6">fers to localising body parts of humans e.g finding the 2D location of the knees, eyes, feet, etc. </span>

</p>
    <p>
    <span class="c28 c6"><br></span><span class="c6">However, 3D Pose Estimation takes this even further by finding the orientation of the body parts in 3D space and then an optional step of shape estimation/modelling can be performed. There has been a tremendous amount of improvement across these sub-domains in the last few years.<br></span>
</p>
<p class="c1"><span class="c6">In terms of competitive evaluation “</span><span class="c7 c6">t</span><span class="c7 c6 c14">he COCO 2016 Keypoint Challenge involves simultaneously detecting people and localizing their keypoints</span><span class="c6 c14">”.</span><sup class="c6 c12"><a href="#ftnt104" id="ftnt_ref104">[104]</a></sup><span class="c6 c14">&nbsp;The European Convention on Computer Vision (ECCV)</span><sup class="c6 c12"><a href="#ftnt105" id="ftnt_ref105">[105]</a></sup><span class="c6 c14">&nbsp;provides more extensive literature on these subjects, however we would like to highlight:</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_al4bo8mwtqn4-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</span><span class="c6">.</span><sup class="c6 c12"><a href="#ftnt106" id="ftnt_ref106">[106]</a></sup><span class="c5">&nbsp;</span></li>
</ul>
<p class="c1 c2"><span class="c6">This method set SOTA performance on the inaugural MSCOCO 2016 keypoints challenge with 60% average precision (AP) and won the best demo award at ECCV, video: </span>
    <span class="c0">
        <a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=pW6nZXeWlGM">Video</a>
    </span><sup class="c6 c12"><a href="#ftnt107" id="ftnt_ref107">[107]</a></sup></p>

    <div class="embed-container">
<iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/pW6nZXeWlGM">
</iframe>
        </div>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_al4bo8mwtqn4-0">
    <li class="c1 c2 c13"><span class="c12 c6">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</span><span class="c6">.</span><sup class="c6 c12"><a href="#ftnt108" id="ftnt_ref108">[108]</a></sup><span class="c6">&nbsp;This method first predicts 2D body joint locations and then uses another model called SMPL to create the 3D body shape mesh, which allows it to understand 3D aspects working from 2D pose estimation. The 3D mesh is capable of capturing both pose and shape, versus previous methods which could only find 2D human pose.</span><span class="c28 c6">&nbsp;</span><span class="c6">The authors provide an excellent video analysis of their work here: </span><span class="c0">
        <a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=eUnZ2rjxGaE">Video</a>

    </span>
        <sup class="c6 c12"><a href="#ftnt109" id="ftnt_ref109">[109]</a></sup><span class="c5">&nbsp;<br></span></li>
</ul>
    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/eUnZ2rjxGaE">
</iframe>
        </div>

<p class="c1" style="margin-top: 20px;"><span class="c6">“</span><span class="c7 c6">We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D</span><span class="c6">”.</span><sup class="c6 c12"><a href="#ftnt110" id="ftnt_ref110">[110]</a></sup><span class="c28 c6"><br></span></p>

<p class="c1 c9"><span class="c5"></span></p>

    <h1 class="c15 main-heading" id="heading-reconstruction"><span class="c12">Reconstruction</span></h1>

<p class="c1"><span class="c6">As mentioned, a previous section presented some examples of reconstruction but with a general focus on objects, specifically their shape and pose. </span><span class="c6">While some of this is technically reconstruction, the field itself comprises many different types of reconstruction, e.g. scene reconstruction, multi-view and single view reconstruction, structure from motion (SfM), SLAM, etc. Furthermore, some reconstruction approaches leverage additional (and multiple) sensors and equipment, such</span><span class="c28 c6">&nbsp;</span><span class="c6">as Event or RGB-D cameras, and can often layer multiple techniques to drive progress.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">The result?</span><span class="c5">&nbsp;Whole scenes can be reconstructed non-rigidly and change spatio-temporally, e.g. a high-fidelity reconstruction of yourself, and your movements, updated in real-time.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">As identified previously, issues persist around the mapping of 2D images to 3D space. The following papers present a plethora of approaches to create high-fidelity, real-time reconstructions:<br></span></p>
<ul class="c18 lst-kix_qhe9txsqma08-0 start" style="margin-top: 12px;">
    <li class="c1 c2 c13"><span class="c12 c6">Fusion4D: Real-time Performance Capture of Challenging Scenes</span><sup class="c12 c6"><a href="#ftnt111" id="ftnt_ref111">[111]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">veers towards the domain of Computer Graphics, however the interplay between Computer Vision and Graphics cannot be </span><span class="c6">overstated</span><span class="c6">. The authors’ approach uses </span><span class="c6">RGB-D</span><span class="c5">&nbsp;and Segmentation as inputs to form a real-time, multi-view reconstruction which is outputted using Voxels.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 13</span><span class="c5">: Fusion4D examples from real-time feed</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 121.33px;"><img alt="" class="border cntr" src="images/c-image13.jpg" style="width: 100%; max-width: 700px;  /*width: 624.00px; height: 121.33px; /*margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1 c2"><span class="c6"><br></span><span class="c3 c12">Note from source</span><span class="c3">: “</span><span class="c7 c3">We present a new method for real-time high quality 4D (i.e. spatio-temporally coherent) performance capture, allowing for incremental non-rigid reconstruction from noisy input from multiple RGBD cameras. Our system demonstrates unprecedented reconstructions of challenging non-rigid sequences, at real-time rates, including robust handling of large frame-to-frame motions and topology changes.</span><span class="c8 c3">”</span></p>

<p class="c1 c2"><span class="c3 c12">Source</span><span class="c3">: Dou et al. (2016, p. 1)</span><sup class="c6 c12"><a href="#ftnt112" id="ftnt_ref112">[112]</a></sup><span class="c3"><br><br></span><span class="c6">Fusion4D creates real-time, high fidelity voxel representations which have impressive applications in virtual reality, augmented reality and telepresence. This work from Microsoft will likely revolutionise motion capture, possibly for live sports. An example of the technology in real-time use is available here: </span><span class="c0"><a class="c4 a-word-wrap" href="https://youtu.be/2dkcJ1YhYw4">Video</a></span>

    <sup class="c6 c12"><a href="#ftnt113" id="ftnt_ref113">[113]</a></sup>
    </p>
<div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/2dkcJ1YhYw4">
</iframe>
    </div>



<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">For an astounding example of telepresence/holoportation by Microsoft, see here: </span><span class="c0">
    <a class="c4 a-word-wrap" href="https://youtu.be/7d59O6cfaM0">Video</a>
</span><sup class="c6"><a href="#ftnt114" id="ftnt_ref114">[114]</a></sup>
</p>

    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/7d59O6cfaM0">
</iframe>
        </div>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_qhe9txsqma08-0">
    <li class="c1 c2 c13"><span class="c12 c6">Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera</span><sup class="c6 c12"><a href="#ftnt115" id="ftnt_ref115">[115]</a></sup>
        <span class="c5">&nbsp;won best paper at the European Convention on Computer Vision (ECCV) in 2016. The authors propose a novel algorithm capable of tracking 6D motion and various reconstructions in real-time using a single Event Camera.</span>
    </li>
</ul>
<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c10 c2"><span class="c12 c6">Figure 14</span><span class="c5">: Examples of the Real-Time 3D Reconstruction </span></p>

<p class="c10 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 312.00px;"><img alt="" class="border cntr" src="images/c-image5.jpg" style="width: 100%; max-width: 624px; /*width: 624.00px; height: 312.00px; /*margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1 c2"><span class="c3 c12">Note from source</span><span class="c8 c3">: Demonstrations in various settings of the different aspects of our joint estimation algorithm. (a) visualisation of the input event stream; (b) estimated gradient keyframes; (c) reconstructed intensity keyframes with super resolution and high dynamic range properties; (d) estimated depth maps; (e) semi-dense 3D point clouds.</span>
</p>

<p class="c1 c2"><span class="c3 c12">Source: </span><span class="c3">Kim et al. (2016, p. 12)</span><sup class="c6 c12"><a href="#ftnt116" id="ftnt_ref116">[116]</a></sup></p>

<p class="c1 c2 c9"><span class="c8 c3"></span></p>

<p class="c1 c2"><span class="c6">The Event camera is gaining favour with researchers in Computer Vision due to its reduced latency, lower power consumption and higher dynamic range when compared to traditional cameras. Instead of a sequence of frames outputted by a regular camera, the event camera outputs “</span><span class="c7 c6">a stream of asynchronous spikes, each with pixel location, sign and precise timing, indicating when individual pixels record a threshold log intensity change.</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt117" id="ftnt_ref117">[117]</a></sup></p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">For an explanation of event camera functionality, real-time 3D reconstruction and 6-DoF tracking, see the paper’s accompanying video here: </span><span class="c0">
    <a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=yHLyhdMSw7w">Video</a></span>

    <sup class="c6 c12"><a href="#ftnt118" id="ftnt_ref118">[118]</a>

    </sup><span class="c5">&nbsp; <br></span></p>

    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/yHLyhdMSw7w">
</iframe>
        </div>

<p class="c1 c2"><span class="c5">This approach is incredibly impressive when one considers the real-time image rendering and depth estimation involved using a single view-point:</span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">We propose a method which can perform real-time 3D reconstruction from a </span><span class="c7 c6 c22">single hand-held event camera</span><span class="c7 c6">&nbsp;with no additional sensing, and works in unstructured scenes of which it has no prior knowledge.</span><span class="c5">” </span></p>

<p class="c1 c9"><span class="c50 c12 c28 c25"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue</span><sup class="c12 c6"><a href="#ftnt119" id="ftnt_ref119">[119]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">proposes an unsupervised method for training a deep CNN for single view depth prediction with results comparable to SOTA using supervised methods. Traditional deep CNN approaches for single view depth prediction require large amounts of manually labelled data, however unsupervised methods again demonstrate their value by removing this necessity. The authors achieve this “</span><span class="c7 c6">by training the network in a manner analogous to an autoencoder</span><span class="c6">”, using a stereo-rig.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-other-uncategorised"><span class="c12">Other uncategorised </span><span>3D</span></h1>
<ul class="c18 lst-kix_wbj7rym4u0qv-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">IM2CAD</span><sup class="c12 c6"><a href="#ftnt120" id="ftnt_ref120">[120]</a></sup><span class="c5">&nbsp;describes the process of transferring an ‘image to CAD model’, CAD meaning computer-assisted design, which is a prominent method used to create 3D scenes for architectural depictions, engineering, product design and many other fields.</span>
    </li>
</ul>
<p class="c1 c2"><span class="c6"><br>“</span><span class="c7 c6 c14">Given a single photo of a room and a large database of furniture CAD models, our goal is to reconstruct a scene that is as similar as possible to the scene depicted in the photograph, and composed of objects drawn from the database.</span><span class="c5 c14">” </span></p>

<p class="c1 c2 c9"><span class="c5 c14"></span></p>

<p class="c1 c2"><span class="c5 c14">The authors present an automatic system which ‘iteratively optimizes object placements and scales’ to best match input from real images. The rendered scenes validate against the original images using metrics trained using deep CNNs. </span>

</p>
    <!--<div class="bullet-point-margin">-->

<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c10"><span class="c12 c6">Figure 15</span><span class="c5">: Example of IM2CAD rendering bedroom scene</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/  margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 517.33px;"><img alt="" class="border cntr" src="images/c-image6.jpg" style="width: 100%; max-width: 624px; /*width: 624.00px; height: 517.33px; /*display: initial !important; margin-left: 0.00px; margin-top: 0.00px; */transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c6"><br></span><span class="c3 c12">Note</span>
<span class="c3">: Left: input image. Right: Automatically created CAD model from input.<br></span><span class="c3 c12">Note from source</span><span class="c8 c3">: The reconstruction results. In each example the left image is the real input image and the right image is the rendered 3D CAD model produced by IM2CAD.</span>
<br><span class="c3 c12">Source</span><span class="c3">: Izadinia et al. (2016, p. 10)</span>
    <sup class="c3 c12"><a href="#ftnt121" id="ftnt_ref121">[121]</a></sup><span class="c8 c3">&nbsp;</span>
</p>



<p class="c1 c2 c9"><span class="c20 c12 c6"></span></p>
<!--</div>-->

<p class="c1 c2"><span class="c12 c6 c14">Why care about IM2CAD?</span><span class="c6"><br>The issue tackled by the authors is one of the first meaningful advancements on the techniques demonstrated by Lawrence Roberts in 1963, which allowed inference of a 3D scene from a photo using a known-object database, albeit in the very simple case of line drawings. </span>
</p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">While Robert’s method was visionary, more than a half century of subsequent research in Computer Vision has still not yet led to practical extensions of his approach that work reliably on realistic images and scenes.</span><span class="c5">”</span></p>

<p class="c1 c2"><span class="c6"><br>The authors introduce a variant of the problem, aiming to reconstruct a high fidelity scene from a photo using ‘</span><span class="c7 c6">objects taken from a database of 3D object models</span><span class="c5">’ for reconstruction. </span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>

<p class="c1 c2"><span class="c12 c6">The process behind IM2CAD is quite involved and includes:</span></p>
<ul class="c18 lst-kix_d2qvshahiupy-0 start" style="list-style-type:circle">
    <li class="c1 c13 c23"><span class="c6">A </span><span class="c6">Fully Convolutional Network that is trained end-to-end to find Geometric Features for </span><span class="c6 c22">Room Geometry Estimation</span><span class="c5">. </span></li>
    <li class="c1 c13 c23"><span class="c6">Faster R-CNN for </span><span class="c6 c22">Object Detection</span><span class="c6">. </span></li>
    <li class="c1 c13 c23"><span class="c6">After finding the objects within the image, </span><span class="c6 c22">CAD Model Alignment</span>
        <span class="c6">&nbsp;is completed to find the closest models within the ShapeNet repository for the detected objects. For example, the type of chair, given shape and approximate 3D pose. Each 3D model is rendered to 32 viewpoints which are then compared with the bounding box generated in object detection using </span>
        <span class="c6 c22">deep features</span>
        <sup class="c6 c12"><a class="a-word-wrap" href="#ftnt122" id="ftnt_ref122">[122]</a></sup><span class="c5">.</span></li>
    <li class="c1 c13 c23"><span class="c6 c22">Object Placement in the Scene</span></li>
    <li class="c1 c13 c23">
        <span class="c6">Finally Scene Optimization further refines the placement of the objects by optimizing the visual similarity between the camera views of the rendered scene and input image.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c20 c12 c6">Again in this domain, ShapeNet proves invaluable:</span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">First, we leverage ShapeNet, which contains millions of 3D models of objects, including thousands of different chairs, tables, and other household items. This dataset is a game changer for 3D scene understanding research, and was key to enabling our work.</span><span class="c5">”</span></p>

<p class="c10 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_wbj7rym4u0qv-0">
    <li class="c1 c2 c13"><span class="c12 c6">Learning Motion Patterns in Videos</span><sup class="c6 c12"><a href="#ftnt123" id="ftnt_ref123">[123]</a></sup><span class="c6">&nbsp;proposes to solve the issue of determining </span><span class="c6 c22">object motion independent of camera movement</span><span class="c6">&nbsp;using synthetic video sequences to teach the networks. “</span><span class="c7 c6">The core of our approach is a fully convolutional network, which is learnt entirely from synthetic video sequences, and their ground-truth optical flow and motion segmentation.</span><span class="c6">” </span><span class="c6">The authors test their approach on the new moving object segmentation dataset called DAVIS,</span><sup class="c6 c12"><a href="#ftnt124" id="ftnt_ref124">[124]</a></sup><span class="c5">&nbsp;as well as the Berkeley motion segmentation dataset and achieve SOTA on both.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_wbj7rym4u0qv-0">
    <li class="c1 c2 c13"><span class="c12 c6">Deep Image Homography Estimation</span><sup class="c12 c6"><a href="#ftnt125" id="ftnt_ref125">[125]</a></sup><span class="c6">&nbsp;comes from the Magic Leap team, a secretive US startup working in Computer Vision and Mixed Reality. The authors reclassify the task of homography estimation as ‘</span><span class="c7 c6">a learning problem</span><span class="c6">’ and present two deep CNNs architectures which form “</span><span class="c7 c6">HomographyNet: a regression network which directly estimates the real-valued homography parameters, and a classification network which produces a distribution over quantized homographies.</span><span class="c5">” </span></li>
</ul>
<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">The term homography comes from projective geometry and refers to a type of transformation that maps one plane to another. ‘</span><span class="c7 c6">Estimating a 2D homography from a pair of images is a fundamental task in computer vision, and an essential part of monocular SLAM systems</span><span class="c5">’.</span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">The authors also provide a method for producing a “</span><span class="c7 c6">seemingly infinite dataset</span><span class="c6">”, from existing datasets of real images such as MS-COCO, which offsets some of data requirements of deeper networks. They manage to create </span><span class="c6">“</span><span class="c7 c6">a nearly unlimited number of labeled training examples by applying random projective transformations to a large image dataset</span><span class="c5">”.<br></span></p>
<ul class="c18 lst-kix_wbj7rym4u0qv-0">
    <li class="c1 c2 c13"><span class="c12 c6">gvnn: Neural Network Library for Geometric Computer Vision</span><sup class="c12 c6"><a href="#ftnt126" id="ftnt_ref126">[126]</a></sup><span class="c6">&nbsp;introduces a new neural network library for Torch, a popular computing framework for machine learning. Gvnn aims to ‘bridge the gap between classic geometric computer vision and deep learning’. The gvnn library allows developers to add geometric capabilities to their existing networks and training methods. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">In this work, we build upon the 2D transformation layers originally proposed in the spatial transformer networks and provide various novel extensions that perform geometric transformations which are often used in geometric computer vision.</span><span class="c5">”<br></span></p>

<p class="c1 c2"><span class="c6">"</span><span class="c7 c6">This opens up applications in learning invariance to 3D geometric transformation for place recognition, end-to-end visual odometry, depth estimation and unsupervised learning through warping with a parametric transformation for image reconstruction error.</span><span class="c5">" </span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-3d-summation"><span>3D summation and SLAM</span></h1>

<p class="c1"><span class="c6">Throughout this section we cut a swath across the field of 3D understanding, focusing primarily on the areas of </span><span class="c6">P</span><span class="c6">ose </span><span class="c6">E</span><span class="c6">stimation, </span><span class="c6">R</span><span class="c6">econstruction, </span><span class="c6">D</span><span class="c6">epth </span><span class="c6">E</span><span class="c6">stimation and </span><span class="c6">H</span><span class="c6">omography. But there is considerably more superb work which will go unmentioned by us, constrained as we are by volume. And so, we hope to have provided the reader with a valuable starting point, which is to say by no means an absolute.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">A large portion of the highlighted work may be classified under Geometric Vision, which generally deals with measuring real-world quantities like distances, shapes, areas and volumes directly from images. Our heuristic is that recognition-based tasks focus more on higher level semantic information than typically concerns applications in Geometric Vision. However, often we find that much of these different areas of 3D understanding are inextricably linked.</span>
</p>

<p class="c1"><span class="c5"><br>One of the largest Geometric problems is that of simultaneous localisation and mapping (SLAM), with researchers considering whether SLAM will be in the next problems tackled by Deep Learning. Skeptics of the so-called ‘universality’
    of deep learning, of which there are many, point to the importance and functionality of SLAM as an algorithm:</span></p>

<p class="c1"><span class="c6"><br>“</span><span class="c7 c6">Visual SLAM algorithms are able to simultaneously build 3D maps of the world while tracking the location and orientation of the camera.</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt127" id="ftnt_ref127">[127]</a></sup><span class="c6">&nbsp;The geometric estimation portion of the SLAM approach is not currently suited to deep learning approaches and end-to-end learning remains unlikely. SLAM represents one of the most important algorithms in robotics and was designed with large input from the Computer Vision field. The technique has found its home in applications like Google Maps, autonomous vehicles, AR devices like Google Tango</span><sup class="c6 c12"><a href="#ftnt128" id="ftnt_ref128">[128]</a></sup><span class="c5">&nbsp;and even the Mars Rover.</span></p>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>

<p class="c1"><span class="c6">That being said, Tomasz Malisiewicz delivers the anecdotal aggregate opinion of some prominent researchers on the issue, who agree “</span><span class="c7 c6">that </span><span class="c7 c6">semantics are necessary to build bigger and better SLAM systems.</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt129" id="ftnt_ref129">[129]</a></sup><span class="c5">&nbsp;This potentially shows promise for future applications of Deep Learning in the SLAM domain.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">We reached out to Mark Cummins, co-founder of Plink and Pointy, who provided us with his thoughts on the issue. Mark completed his PhD on SLAM techniques:</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">“</span><span class="c7 c6 c27">The core geometric estimation part of SLAM is pretty well solved by the current approaches, but the high-level semantics and the lower-level system components can all benefit from deep learning. In particular:<br></span>
</p>
    <br>
<ul class="c18 lst-kix_gi47s7ptc0lo-0 start">
    <li class="c1 c2 c13"><span class="c27 c7 c6">Deep learning can greatly improve the quality of map semantics - i.e. going beyond poses or point clouds to a full understanding of the different kind of objects or regions in the map. This is much more powerful for many applications, and can also help with general robustness (for example through better handling of dynamic objects and environmental changes).<br></span>
    </li>
</ul>
    <br>
<ul class="c18 lst-kix_xeitgb3brsv-0 start">
    <li class="c1 c2 c13"><span class="c27 c7 c6">At a lower level, many components can likely be improved via deep learning. Obvious candidates are place recognition / loop closure detection / relocalization, better point descriptors for sparse SLAM methods, etc<br></span>
    </li>
</ul>
    <br>
<p class="c1"><span class="c7 c6">Overall the structure of SLAM solvers probably remains the same, but the components improve. It is possible to imagine doing something radically new with deep learning, like throwing away the geometry entirely and have a more recognition-based navigation system. But for systems where the goal is a precise geometric map, deep learning in SLAM is likely more about improving components than doing something completely new.</span><span class="c5">”</span></p>

<p class="c1"><span class="c6"><br>In summation, we believe that SLAM is not likely to be completely</span><span class="c28 c6">&nbsp;</span><span class="c6">replaced by Deep Learning. However, it is entirely likely that the two approaches may become complements to each other going forward. If you wish to learn more about SLAM, and its current SOTA, we wholeheartedly recommend Tomasz Malisiewicz’s blog for that task: </span><span class="c0"><a class="c4 a-word-wrap" href="http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html">The Future of
    Real-Time SLAM and Deep Learning vs SLAM</a></span><sup class="c6 c12"><a href="#ftnt130" id="ftnt_ref130">[130]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

    <h1 class="c15 main-heading" id="heading-part-four"><span class="c20 c12 c25">Part Four: ConvNet Architectures, Datasets, Ungroupable Extras</span></h1>

<h1 class="c15 main-heading" id="heading-convnet-arch"><span class="c12 c25">ConvNet Architectures</span></h1><span class="c25"><br></span><span class="c32 c6">ConvNet architectures have recently found many novel applications outside of Computer Vision, some of which will feature in our forthcoming publications. However, they continue to feature prominently in</span>
    <span class="c32 c6">Computer Vision, with architectural advancements providing improvements in speed, accuracy and training for many of the aforementioned applications and tasks in this paper. </span>

    <span class="c32"><br><br></span>
<span class="c5">For this reason, ConvNet architectures are of fundamental importance to Computer Vision as a whole. The following features some noteworthy ConvNet architectures from 2016, many of which take inspiration from the recent success of ResNets. </span>
<ul class="c18 lst-kix_jiwkhisoudvb-0" style="margin-top: 10px;">
    <li class="c1 c2 c13"><span class="c12 c6">Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning</span><sup class="c12 c6"><a href="#ftnt131" id="ftnt_ref131">[131]</a></sup><span class="c6">&nbsp;- present Inception v4, a new Inception architecture which builds on the Inception v2 and v3 from the end of 2015.</span><sup class="c6 c12"><a href="#ftnt132" id="ftnt_ref132">[132]</a></sup><span class="c5">&nbsp;The paper also provides an analysis of using residual connections for training Inception Networks along with some Residual-Inception hybrid networks. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_nfilvffzvtk4-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Densely Connected Convolutional Networks</span><sup class="c12 c6"><a href="#ftnt133" id="ftnt_ref133">[133]</a></sup><span class="c6">&nbsp;or</span><span class="c6">&nbsp;“</span><span class="c6">DenseNets</span><span class="c5">” take direct inspiration from the identity/skip connections of ResNets. The approach extends this concept to ConvNets by having each layer connect to every other layer in a feed forward fashion, sharing feature maps from previous layers as inputs, thus creating DenseNets. </span>
    </li>
</ul>
<p class="c1 c2"><span class="c6"><br>“</span><span class="c7 c6">DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters</span><span class="c6">”.</span><sup class="c6 c12"><a href="#ftnt134" id="ftnt_ref134">[134]</a></sup><span class="c5">&nbsp;<br></span></p>

<p class="c10" style="margin-top: 10px;"><span class="c12 c6">Figure 16</span><span class="c5">: Example of DenseNet Architecture</span></p>

<p class="c10 c2 image-margin">
    <!--<span class="c12 c6">Figure 16</span><span class="c6">: Example of DenseNet Architecture</span><span class="c6"><br></span>-->
    <span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 472.00px; height: 338.75px;"><img alt="" class="border cntr" src="images/image14.png" style="width: 100%; max-width: 472px; /*width: 472.00px; height: 338.75px; /*margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1 c2"><span class="c3 c12">Note</span><span class="c3">: A 5-layer dense block with a growth rate of </span><span class="c7 c3">k = 4</span><span class="c8 c3">. Each layer takes all preceding feature-maps as input.</span></p>

<p class="c1 c2"><span class="c3 c12">Source</span><span class="c3">: Huang et al. (2016)</span><sup class="c6 c12"><a href="#ftnt135" id="ftnt_ref135">[135]</a></sup></p>

<p class="c10 c2 c9"><span class="c8 c3"></span></p>

<p class="c1 c2"><span class="c6">The model was evaluated on CIFAR-10, CIFAR-100, SVHN and ImageNet; it achieved SOTA on a number of them. Impressively, DenseNets achieve these results while using less memory and with reduced computational requirements. There are multiple implementations (Keras, Tensorflow, etc) </span><span class="c0"><a class="c4 a-word-wrap" href="https://github.com/liuzhuang13/DenseNet">here</a></span><span class="c6">.</span><sup class="c6 c12"><a href="#ftnt136" id="ftnt_ref136">[136]</a></sup></p>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_bgc1ik7oba91-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">FractalNet</span><span class="c6">&nbsp;</span><span class="c12 c6">Ultra-Deep Neural Networks without Residuals</span><sup class="c12 c6"><a href="#ftnt137" id="ftnt_ref137">[137]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">-</span><span class="c5">&nbsp;utilises interacting subpaths of different lengths, without pass-through or residual connections, instead altering internal signals using filter and nonlinearities for transformations. </span>
    </li>
</ul>
<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">FractalNets repeatedly combine several parallel layer sequences with different numbers of convolutional blocks to obtain a large nominal depth, while maintaining many short paths in the network</span><span class="c6">”.</span><sup class="c6 c12"><a href="#ftnt138" id="ftnt_ref138">[138]</a></sup></p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c5">The network achieved SOTA performance on CIFAR and ImageNet, while demonstrating some additional properties. For instance, they call into question the role of residuals in the success of extremely deep ConvNets, while also providing insight into the nature of answers attained by various subnetwork depths.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_nfilvffzvtk4-0">
    <li class="c1 c2 c13"><span class="c12 c6">Lets keep it simple: using simple architectures to outperform deeper architectures</span><sup class="c12 c6"><a href="#ftnt139" id="ftnt_ref139">[139]</a></sup><span class="c5">&nbsp;focuses on creating a simplified mother architecture. The architecture achieved SOTA results, or parity with existing approaches, on ‘datasets such as CIFAR10/100, MNIST and SVHN with simple or no data-augmentation’. We feel their exact words provide the best description of the motivation here:<br></span>
    </li>
</ul>
<p class="c1 c2"><span class="c6">“</span><span class="c27 c7 c6">In this work, we present a very simple fully convolutional network architecture of 13 layers, with minimum reliance on new features which outperforms almost all deeper architectures with 2 to 25 times fewer parameters. Our architecture can be a very good candidate for many scenarios, especially for use in embedded devices.”</span>
</p>

<p class="c1 c2 c9"><span class="c27 c7 c6"></span></p>

<p class="c1 c2"><span class="c7 c6">“It can be furthermore compressed using methods such as DeepCompression and thus its memory consumption can be decreased drastically. We intentionally tried to create a mother architecture with minimum reliance on new features proposed recently, to show the effectiveness of a well-crafted yet simple convolutional architecture which can then later be enhanced with existing or new methods presented in the literature.</span><span class="c6">”</span><sup class="c6 c12"><a href="#ftnt140" id="ftnt_ref140">[140]</a></sup></p>

<p class="c1 c9"><span class="c29 c28 c6"></span></p>

<p class="c1"><span class="c5">Here are some additional techniques which complement ConvNet Architectures:</span></p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_nfilvffzvtk4-0">
    <li class="c1 c2 c13"><span class="c12 c6">Swapout: Learning an ensemble of deep architectures</span><sup class="c12 c6"><a href="#ftnt141" id="ftnt_ref141">[141]</a></sup><span class="c6">&nbsp;generalises dropout and stochastic depth methods to prevent co-adaptation of units, both in a specific layer and across network layers. The ensemble training method samples from multiple architectures including “</span><span class="c7 c6">dropout, stochastic depth and residual architectures</span><span class="c5">”. Swapout outperforms ResNets of identical network structure on the CIFAR-10 and CIFAR-100 and can be classified as a regularisation technique.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_nfilvffzvtk4-0">
    <li class="c1 c2 c13"><span class="c12 c6">SqueezeNet</span><sup class="c12 c6"><a href="#ftnt142" id="ftnt_ref142">[142]</a></sup><span class="c12 c6">&nbsp;</span><span class="c5">posits that smaller DNNs offer various benefits, from less computationally taxing training to easier information transmission to, and operation on, devices with limited storage or processing power. SqueezeNet is a small DNN architecture which achieves ‘AlexNet-level accuracy with significantly reduced parameters and memory requirements using model compression techniques which make it 510x smaller than AlexNet.’</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">A Rectified Linear Unit (ReLU) is traditionally the dominant activation function for all Neural Networks. However, here are some recent alternatives:<br></span></p>
<ul class="c18 lst-kix_kg9o9flo8md3-0 start" style="margin-bottom:15px;">
    <li class="c1 c2 c13"><span class="c12 c6">Concatenated Rectified Linear Units (CRelu)</span><sup class="c6 c12"><a href="#ftnt143" id="ftnt_ref143">[143]</a></sup></li>
    <li class="c1 c2 c13"><span class="c12 c6">Exponential Linear Units (ELUs)</span><sup class="c6 c12"><a href="#ftnt144" id="ftnt_ref144">[144]</a></sup><span class="c5">&nbsp;from the close of 2015</span></li>
    <li class="c1 c2 c13"><span class="c12 c6">Parametric Exponential Linear Unit (PELU)</span><sup class="c6 c12"><a href="#ftnt145" id="ftnt_ref145">[145]</a></sup><span class="c5">&nbsp;</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c20 c12 c6">Moving towards equivariance in ConvNets</span></p>

<p class="c1"><span class="c5">ConvNets are translation invariant - meaning they can identify the same features in multiple parts of an image. However, the typical CNN isn’t rotation invariant - meaning that if a feature or the whole image is rotated then the network’s performance suffers. Usually ConvNets learn to (sort of) deal with rotation invariance through data augmentation (e.g. purposefully rotating the images by small random amounts during training). This means the network gains slight rotation invariant properties without specifically designing rotation invariance into the network. This means that rotation invariance is fundamentally limited in networks using current techniques. This is an interesting parallel with humans who also typically fare worse at recognising characters upside down, although there is no reason for machines to suffer this limitation.</span>
</p>

<p class="c1"><span class="c6"><br>The following papers </span><span class="c6">tackle</span><span class="c6">&nbsp;</span><span class="c12 c6">rotation-invariant ConvNets</span><span class="c6">. While each approach has novelties, they all improve rotation invariance through more efficient parameter usage leading to eventual global rotation equivariance:</span><span class="c5"><br></span></p>
<ul class="c18 lst-kix_t4jp9ig9481p-0 start">
    <li style="margin-bottom:10px;" class="c1 c2 c13"><span class="c12 c6">Harmonic CNNs</span><sup class="c6 c12"><a href="#ftnt146" id="ftnt_ref146">[146]</a></sup><span class="c5">&nbsp;replace regular CNN filters with ‘circular harmonics’. <br></span></li>
    <li style="margin-bottom:10px;" class="c1 c2 c13"><span class="c12 c6">Group Equivariant Convolutional Networks (G-CNNs)</span><sup class="c6 c12"><a href="#ftnt147" id="ftnt_ref147">[147]</a></sup><span class="c6">&nbsp;</span><span class="c6">uses G-Convolutions, which</span><span class="c28 c6">&nbsp;</span><span class="c6">are a new type of layer that “</span><span class="c7 c6">enjoys a substantially higher degree of weight sharing than regular convolution layers and increases the expressive capacity of the network without increasing the number of parameters.</span><span class="c5">”<br></span></li>
    <li style="margin-bottom:10px;" class="c1 c2 c13"><span class="c12 c6">Exploiting Cyclic Symmetry in Convolutional Neural Networks</span><sup class="c12 c6"><a href="#ftnt148" id="ftnt_ref148">[148]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">presents four operations as layers which augment neural network layers to partially increase rotational equivariance. <br></span>
    </li>
    <li style="margin-bottom:10px;" class="c1 c2 c13"><span class="c12 c6">Steerable CNNs</span><sup class="c12 c6"><a href="#ftnt149" id="ftnt_ref149">[149]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">-</span><span class="c12 c6">&nbsp;</span><span class="c6">Cohen and Welling build on the work they did with </span><span class="c12 c6">G-CNNs</span><span class="c6">, demonstrating that “</span><span class="c7 c6">steerable architectures” </span><span class="c6">outperform residual and dense networks</span><span class="c5">&nbsp;on the CIFARs. They also provide a succinct overview of the invariance problem: </span>
    </li>
</ul>
<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">“</span><span class="c7 c6">To improve the statistical efficiency of machine learning methods, many have sought to learn invariant representations. In deep learning, however, intermediate layers should not be fully invariant, because the relative pose of local features must be preserved for further layers. Thus, one is led to the idea of </span><span class="c7 c12 c6">equivariance</span><span class="c7 c6">: a network is equivariant if the representations it produces transform in a predictable linear manner under transformations of the input. In other words, equivariant networks produce representations that are steerable. Steerability makes it possible to apply filters not just in every position (as in a standard convolution layer), but in every pose, thus allowing for increased parameter sharing.</span><span class="c6">”</span><span class="c6 c45">107</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c20 c12 c25"></span></p>

<p class="c1"><span class="c20 c12 c25">Residual Networks</span></p>

<p class="c1 c9"><span class="c20 c12 c25"></span></p>

<p class="c10"><span class="c12 c6">Figure 17</span><span class="c5">: Test-Error Rates on CIFAR Datasets</span></p>

<p class="c10 image-margin"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); /*width: 505.00px; height: 442.00px;*/"><img alt="Screenshot 2017-03-07 15.37.19.png" src="images/c-image2.jpg" style="width: 90%; max-width: 505px; /*width: 505.00px; height: 442.00px; margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);     border: 0px !important;" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c3">: Yellow highlight indicates that these papers feature within this piece. Pre-resnet refers to "</span><span class="c7 c3">Identity Mappings in Deep Residual Networks</span><span class="c3">" (see following section). Furthermore, while not included in the table we believe that </span><span class="c7 c3">“</span><span class="c7 c3 c12">Learning Identity Mappings with Residual Gates</span><span class="c8 c3">”
    produced some of the lowest error rates of 2016 with 3.65% and 18.27% on CIFAR-10 and CIFAR-100, respectively.</span></p>

<p class="c1"><span class="c3 c12">Source</span><span class="c3">: Abdi and Nahavandi (2016, p. 6)</span><sup class="c6 c12"><a href="#ftnt150" id="ftnt_ref150">[150]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">Residual Networks and their variants became incredibly popular in 2016, following the success of Microsoft’s ResNet,</span><sup class="c6 c12"><a href="#ftnt151" id="ftnt_ref151">[151]</a></sup><span class="c6">&nbsp;with many open source versions and pre-trained models now available. In 2015, ResNet won 1</span><span class="c6 c45">st </span><span class="c6">place in ImageNet’s Detection, Localisation and Classification tasks as well as in COCO’s Detection and Segmentation challenges.</span><span class="c28 c6">&nbsp;</span><span class="c6">Although questions still abound about depth, ResNets tackling of the vanishing gradient problem provided more impetus for the “</span><span class="c7 c6">increased depth produces superior abstraction</span><span class="c5">” philosophy which underpins much of Deep Learning at present.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">ResNets are often conceptualised as an ensemble of shallower networks, which somewhat counteract the hierarchical nature of Deep Neural Networks (DNNs) by running shortcut connections parallel to their convolutional layers. These shortcuts or </span><span class="c7 c12 c6">skip connections</span><span class="c6">&nbsp;mitigate vanishing/exploding gradient problems associated with DNNs, by allowing easier back-propagation of gradients throughout the network layers. For more information there is a Quora thread available </span><span class="c0"><a class="c4" href="https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks">here</a></span><span class="c6">.</span><sup class="c6 c12"><a href="#ftnt152" id="ftnt_ref152">[152]</a></sup></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c12 c6">Residual Learning</span><span class="c20 c12 c6">, Theory and Improvements</span></p>

<p class="c1 c9"><span class="c20 c12 c6"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">Wide Residual Networks</span><sup class="c12 c6"><a href="#ftnt153" id="ftnt_ref153">[153]</a></sup><span class="c6">&nbsp;is now an extremely common ResNet approach. The authors conduct an experimental study on the architecture of ResNet blocks, and improve residual network performance by increasing the width and reducing the depth of the networks, which mitigates the diminishing feature reuse problem. This approach produces new SOTA on multiple benchmarks including 3.89% and 18.3% on CIFAR-10 and CIFAR-100 respectively. The authors show that a ‘</span><span class="c5">16-layer-deep wide ResNet performs as well or better in accuracy and efficiency than many other ResNets (including 1000 layer networks)’. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">Deep Networks with Stochastic Depth</span><sup class="c12 c6"><a href="#ftnt154" id="ftnt_ref154">[154]</a></sup><span class="c6">&nbsp;essentially applies dropout to whole layers of neurons instead of to bunches of </span><span class="c6">individual </span><span class="c6">neurons. “</span><span class="c7 c6">We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function.</span><span class="c6">” </span><span class="c6">Stochastic depth allows quicker training and better accuracy even when training networks greater than 1200 layers.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">Learning Identity Mappings with Residual Gates</span><sup class="c12 c6"><a href="#ftnt155" id="ftnt_ref155">[155]</a></sup><span class="c6">&nbsp;- </span><span class="c6">“</span><span class="c7 c6">by using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter.</span><span class="c6">” The authors use these Gated ResNets to improve the optimisation of deep models, while providing ‘high tolerance to full layer removal’
        such that 90% of performance remains following significant removal at random. Using Wide Gated ResNets the model achieves 3.65% and 18.27% error on CIFAR- 10 and CIFAR-100, respectively. </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</span><sup class="c12 c6"><a href="#ftnt156" id="ftnt_ref156">[156]</a></sup><span class="c12 c6">&nbsp;</span><span class="c6">- </span><span class="c5">ResNets can be viewed as collections of many paths, which don’t strongly depend upon one another and hence reinforce the notion of ensemble behaviour. Furthermore, residual pathways vary in length with the short paths contributing to gradient during training while the deeper paths don’t factor in this stage. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">Identity Mappings in Deep Residual Networks</span><sup class="c12 c6"><a href="#ftnt157" id="ftnt_ref157">[157]</a></sup><span class="c6">&nbsp;comes as an improvement from the original Resnet authors: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. Identity mappings are shown to allow ‘forward and backward signals to be propagated between any ResNet block when used as the skip connections and after-addition activation’. The approach improves generalisation, training and results “</span><span class="c7 c6">using a 1001-layer ResNet on CIFAR-10 (4.62% error) and CIFAR-100, and a 200-layer ResNet on ImageNet.</span><span class="c5">” </span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c12 c6">M</span><span class="c12 c6">ulti-Residual Networks: Improving the Speed and Accuracy of Residual Networks</span><sup class="c12 c6"><a href="#ftnt158" id="ftnt_ref158">[158]</a></sup><span class="c6">&nbsp;again advocates for the ensemble behaviour of ResNets and favours a wider-over-deeper approach to ResNet architecture. “</span><span class="c7 c6">The proposed multi-residual network increases the number of residual functions in the residual blocks.</span><span class="c6">” Improved accuracy produces 3.73% and 19.45% error on CIFAR-10 and CIFAR-100, respectively. The table presented in Fig. 17 was taken from this paper, and more up-to-date versions are available which consider the work produced in 2017 thus far.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c12 c6">Other residual theory and improvements</span><span class="c12 c28 c6"><br></span><span class="c5">Although a relatively recent idea, there is quite a considerable body of work being created around ResNets presently. The following represents some additional theories and improvements which we wished to highlight for interested readers:</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_sf6r0j5t2h4q-0 start">
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1612.07771">Highway and Residual Networks learn Unrolled
        Iterative Estimation</a></span><sup class="c6 c12"><a href="#ftnt159" id="ftnt_ref159">[159]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1609.05672">Residual Networks of Residual Networks:
        Multilevel Residual Networks</a></span><sup class="c6 c12"><a href="#ftnt160" id="ftnt_ref160">[160]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1603.08029">Resnet in Resnet: Generalizing Residual
        Architectures</a></span><sup class="c6 c12"><a href="#ftnt161" id="ftnt_ref161">[161]</a></sup><span class="c5">&nbsp;</span></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1611.10080">Wider or Deeper: Revisiting the ResNet Model
        for Visual Recognition</a></span><sup class="c6 c12"><a href="#ftnt162" id="ftnt_ref162">[162]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1604.03640">Bridging the Gaps Between Residual Learning,
        Recurrent Neural Networks and Visual Cortex</a></span><sup class="c6 c12"><a href="#ftnt163" id="ftnt_ref163">[163]</a></sup></li>
</ul>
<ul class="c18 lst-kix_jiwkhisoudvb-0">
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1606.05262">Convolutional Residual Memory
        Networks</a></span><sup class="c6 c12"><a href="#ftnt164" id="ftnt_ref164">[164]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1611.04231">Identity Matters in Deep
        Learning</a></span><sup class="c6 c12"><a href="#ftnt165" id="ftnt_ref165">[165]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1604.04112">Deep Residual Networks with Exponential Linear
        Unit</a></span><sup class="c6 c12"><a href="#ftnt166" id="ftnt_ref166">[166]</a></sup></li>
    <li class="c1 c2 c13"><span class="c0"><a class="c4" href="https://arxiv.org/abs/1605.08831">Weighted Residuals for Very Deep
        Networks</a></span><sup class="c6 c12"><a href="#ftnt167" id="ftnt_ref167">[167]</a></sup></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<h1 class="c15 main-heading" id="heading-datasets"><span class="c20 c12 c6">Datasets</span></h1>

<p class="c1"><span class="c6 c26 c14">The significance of rich datasets for all facets of machine learning cannot be overstated. Hence, we feel it is prudent to include some of the largest advancements in this domain. To paraphrase Ben Hamner, the </span><span class="c6 c14">CTO and co-founder of</span><span class="c28 c6 c14">&nbsp;</span><span class="c6 c26 c14">Kaggle, ‘</span><span class="c7 c6">a new dataset can make a thousand papers flourish</span><span class="c6">’</span><span class="c6 c26 c14">,</span><sup class="c6 c12"><a href="#ftnt168" id="ftnt_ref168">[168]</a></sup><span class="c6">&nbsp;</span><span class="c5">that is to say the availability of data can promote new approaches, as well as breath new life into previously ineffectual techniques. </span>
</p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>

<p class="c1"><span class="c6 c26 c14">In 2016, traditional datasets such as ImageNet</span><sup class="c6 c12"><a href="#ftnt169" id="ftnt_ref169">[169]</a></sup><span class="c6 c26 c14">, Common Objects in Context (COCO)</span><sup class="c6 c12"><a href="#ftnt170" id="ftnt_ref170">[170]</a></sup><span class="c6 c26 c14">, the CIFARs</span><sup class="c6 c12"><a href="#ftnt171" id="ftnt_ref171">[171]</a></sup><span class="c6 c26 c14">&nbsp;and MNIST</span><sup class="c6 c12"><a href="#ftnt172" id="ftnt_ref172">[172]</a></sup><span class="c6 c26 c14">&nbsp;were joined by a host of new entries. We also noted the rise of synthetic datasets spurred on by progress in graphics. Synthetic datasets are an interesting work-around of the large data requirements for Artificial Neural Networks (ANNs). In the interest of brevity, we have selected our (subjective) most important </span><span class="c7 c6 c26 c14">new</span><span class="c29 c6 c26 c14">&nbsp;datasets for 2016:</span></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0 start">
    <li class="c1 c2 c13"><span class="c12 c6 c14">Places2</span><sup class="c12 c6 c14"><a href="#ftnt173" id="ftnt_ref173">[173]</a></sup><span class="c6 c14">&nbsp;is a scene classification dataset, i.e. the task is to label an image with a scene class like ‘Stadium’, ‘Park’, etc. While prediction models and image understanding will undoubtedly be improved by the Places2 dataset, an interesting finding from networks that are trained on this dataset is that in the process of learning to classify scenes, the network learns to detect objects in them </span><span class="c6 c14 c22">without ever being explicitly taught this</span><span class="c6 c14">. For example, that bedrooms contain beds and that sinks can be in both kitchens and bathrooms. This means that </span><span class="c6 c14 c22">the objects themselves</span><span class="c5 c14">&nbsp;are lower level features in the abstraction hierarchy for the classification of scenes.</span></li>
</ul>
<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c10"><span class="c12 c6 c14">Figure 18</span><span class="c5 c14">: Examples from SceneNet RGB-D</span></p>

<p class="c10 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); /*width: 624.00px; height: 490.67px;*/"><img alt="" class="border cntr" src="images/c-image15.jpg" style="width: 90%; max-width: 624px; /*width: 624.00px; height: 490.67px;/* margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12 c26 c14">Note</span><span class="c3 c26 c14">: Examples taken from SceneNet RGB-D, a dataset with 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth. The photo (a) is rendered through computer graphics with available ground truth for specific tasks from (b) to (e). Creation of synthetic datasets should aid the process of domain adaptation. </span><span class="c8 c3 c14">Synthetic datasets are somewhat pointless if the knowledge learned from them cannot be applied to the real world. This is where domain adaptation comes in, which refers to this transfer learning process of moving knowledge from one domain to another, e.g. from synthetic to real-world environments. Domain adaptation has recently been improving very rapidly again highlighting the recent efforts in transfer learning. Columns (c) vs (d) show the difference between instance and semantic/class segmentation.</span>
</p>

<p class="c1"><span class="c3 c12 c26 c14">Source</span><span class="c3 c26 c14">: McCormac et al. (2017)</span><sup class="c3 c12"><a href="#ftnt174" id="ftnt_ref174">[174]</a></sup></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0">
    <li class="c1 c2 c13"><span class="c12 c6 c14">SceneNet RGB-D</span><sup class="c12 c6 c14"><a href="#ftnt175" id="ftnt_ref175">[175]</a></sup><span class="c12 c6 c14">&nbsp;</span><span class="c5 c14">- This synthetic dataset expands on the original SceneNet dataset and provides pixel-perfect ground truth for scene understanding problems such as semantic segmentation, instance segmentation, and object detection, and also for geometric computer vision problems such as optical flow, depth estimation, camera pose estimation, and 3D reconstruction. The dataset granularizes the chosen environment by providing pixel-perfect representations. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0">
    <li class="c1 c2 c13"><span class="c12 c6 c14">CMPlaces</span><sup class="c12 c6 c14"><a href="#ftnt176" id="ftnt_ref176">[176]</a></sup><span class="c5 c14">&nbsp;is a cross-modal scene dataset from MIT. The task is to recognize scenes across many different modalities beyond natural images and in the process hopefully transfer that knowledge across modalities too. Some of the modalities are: Real, Clip Art, Sketches, Spatial Text (words written which correspond to spatial locations of objects) and natural language descriptions. The paper also discusses methods for how to deal with this type of problem with cross-modal convolutional neural networks. </span>
    </li>
</ul>

<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c10"><span class="c12 c6 c14">Figure 19</span><span class="c5 c14">: CMPlaces cross-modal scene representations</span></p>

<p class="c1 image-margin"><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); /*width: 624.00px; height: 290.67px;*/">
    <img alt="" class="border cntr" src="images/c-image3-40.jpg" style="width: 95%; max-width: 740px; /*width: 624.00px; height: 290.67px; /*margin-left: 0.00px; margin-top: 0.00px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

    <div class="bullet-point-margin">
<p class="c1"><span class="c3 c12 c26 c14">Note</span><span class="c29 c3 c26 c14">: Taken from the CMPlaces paper showing two examples, bedrooms and kindergarten classrooms, &nbsp; across different modalities. Conventional Neural Network approaches learn representations that don’t transfer well across modalities and this paper attempts to generate a shared representation “agnostic of modality”. </span>
</p>

<p class="c1"><span class="c3 c12 c26 c14">Source</span><span class="c3 c26 c14">: Aytar et al. (2016)</span><sup class="c3 c12"><a href="#ftnt177" id="ftnt_ref177">[177]</a></sup></p>

<p class="c1 c9"><span class="c20 c12 c6 c14"></span></p>
</div>
<p class="c1 c2"><span class="c6 c14">In CMPlaces we see explicit mention of transfer learning, domain invariant representations, domain adaptation and multi-modal learning, all of which serve to demonstrate further the current undertow of Computer Vision research. The authors focus on trying to find </span><span class="c6 c14">“</span><span class="c7 c6 c14">domain/modality-independent representations</span><span class="c6 c14">”, which could correspond to the higher level abstractions where humans draw their unified representations from. For instance take ‘cat’
    across its various modalities, humans see the word ‘cat’ in writing, a picture drawn in a sketchbook, a real world-image or mentioned in speech but we still have the same unified representation abstracted at a higher level above these modalities.</span>
</p>

<p class="c1 c2 c9"><span class="c27 c7 c6 c14"></span></p>

<p class="c1 c2"><span class="c7 c6 c14">“Humans are able to leverage knowledge and experiences independently of the modality they perceive it in, and a similar capability in machines would enable several important applications in retrieval and recognition”</span><span class="c6 c14">.</span><span class="c29 c28 c6 c14">&nbsp;</span></p>

<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0">
    <li class="c1 c2 c13"><span class="c12 c6 c14">MS-Celeb-1M</span><sup class="c12 c6 c14"><a href="#ftnt178" id="ftnt_ref178">[178]</a></sup><span class="c6 c14">&nbsp;contains images of one million celebrities with ten million training images in a training set for Facial Recognition.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0">
    <li class="c1 c2 c13"><span class="c12 c6 c14">Open Images</span><sup class="c12 c6 c14"><a href="#ftnt179" id="ftnt_ref179">[179]</a></sup><span class="c6 c14">&nbsp;</span><span class="c6 c14">comes courtesy of Google Inc. and comprises ~9 million URLs to images complete with multiple labels, a vast improvement over typical single label images. Open images spans 6000 categories, a large improvement over the 1000 classes offered previously by ImageNet (with less focus on canines) and should prove indispensable to the Machine Learning community.</span>
    </li>
</ul>
<p class="c1 c9"><span class="c29 c6 c26 c14"></span></p>
<ul class="c18 lst-kix_d8mii7sh8xk5-0">
    <li class="c1 c2 c13"><span class="c12 c6 c14">YouTube-8M</span><sup class="c12 c6 c14"><a href="#ftnt180" id="ftnt_ref180">[180]</a></sup><span class="c12 c6 c14">&nbsp;</span><span class="c6 c14">also comes courtesy of Google with 8 million video URLs, 500,000 hours of video, 4800 classes, Avg. 1.8 Labels per video. Some examples of the labels are: ‘Arts &amp; Entertainment’, ‘Shopping’
        and ‘Pets &amp; Animals’. Video datasets are much more difficult to label and collect hence the massive value this dataset provides.</span></li>
</ul>
<p class="c1 c9"><span class="c5 c14"></span></p>

<p class="c1 c2"><span class="c6 c14">That being said, a</span><span class="c6 c14">dvancements in image understanding, such as segmentation, object classification and detection have brought video understanding to the fore of research. However, prior to this dataset release there was a real lack in the variety and scale of real-world video datasets available.</span><span class="c6 c14">&nbsp;Furthermore, this dataset was just recently updated,</span><sup class="c6 c12"><a href="#ftnt181" id="ftnt_ref181">[181]</a></sup><span class="c6 c14">&nbsp;and this year in association with Kaggle, Google </span><span class="c6 c14">is</span><span class="c6 c14">&nbsp;organis</span><span class="c6 c14">ing</span><span class="c6 c14">&nbsp;a video understanding competition as part of CVPR 2017.</span><sup class="c6 c12"><a href="#ftnt182" id="ftnt_ref182">[182]</a></sup>
</p>

<p class="c1 c2 c9"><span class="c5 c14"></span></p>

<p class="c1 c2"><span class="c6 c14">General information about YouTube-8M: </span><span class="c0 c14"><a class="c4" href="https://research.google.com/youtube8m/">here</a></span><sup class="c6 c12"><a href="#ftnt183" id="ftnt_ref183">[183]</a></sup></p>

<p class="c1 c9"><span class="c5 c14"></span></p>

<h1 class="c15 main-heading" id="heading-ungroupable"><span class="c20 c12 c25">Ungroupable extras and interesting trends</span></h1>

<p class="c1"><span class="c32 c6">As this piece draws to a close, we lament the limitations under which we had to construct it. Indeed, the field of Computer Vision is too expansive to cover in any real, meaningful depth, and as such many omissions were made. One such omission is, unfortunately, almost everything that </span><span class="c6 c22 c32">didn’t use Neural Networks</span><span class="c32 c6">. We know there is great work outside of NNs, and we acknowledge our own biases, but we feel that the impetus lies with these approaches currently, and our subjective selection of material for inclusion was predominantly based on the reception received from the research community at large</span><span class="c6">&nbsp;(and the results speak for themselves)</span><span class="c32 c6">.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c32 c6">We would also like to </span><span class="c32 c6 c22">stress</span><span class="c32 c6">&nbsp;that there are </span><span class="c32 c6 c22">hundreds of other papers</span><span class="c5">&nbsp;in the above topics, and this amalgam of topics is not curated as a definitive, but rather hopes to encourage interested parties to read further along the entrances we provide. As such, this final section acts as a catch all for some of the other applications we loved, trends we wished to highlight and justifications we wanted to make to the reader.</span>
</p>

<p class="c1 c9"><span class="c8 c33"></span></p>

<p class="c1 c9"><span class="c8 c33"></span></p>

<p class="c1"><span class="c20 c12 c6">Applications/use cases</span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0 start">
    <li class="c1 c2 c13"><span class="c6">Applications for the blind from Facebook</span><sup class="c6 c12"><a href="#ftnt184" id="ftnt_ref184">[184]</a></sup><span class="c6">&nbsp;and hardware from Baidu.</span><sup class="c6 c12"><a href="#ftnt185" id="ftnt_ref185">[185]</a></sup>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Emotion detection combines facial detection and semantic analysis, and is growing rapidly. There are 20+ APIs currently available.</span><sup class="c6 c12"><a href="#ftnt186" id="ftnt_ref186">[186]</a></sup><span class="c6">&nbsp;</span><span class="c5"><br></span></li>
    <li class="c1 c2 c13"><span class="c6">Extracting roads from aerial imagery,</span><sup class="c6 c12"><a href="#ftnt187" id="ftnt_ref187">[187]</a></sup><span class="c6">&nbsp;land use classification from aerial maps and population density maps.</span><sup class="c6 c12"><a href="#ftnt188" id="ftnt_ref188">[188]</a></sup><span class="c5">&nbsp;</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Amazon Go further raised the profile of Computer Vision by demonstrating a </span><span class="c6 c22">queue-less shopping experience</span><span class="c6">,</span><sup class="c6 c12"><a href="#ftnt189" id="ftnt_ref189">[189]</a></sup><span class="c6">&nbsp;although there remain some functional issues at present.</span><sup class="c6 c12"><a href="#ftnt190" id="ftnt_ref190">[190]</a></sup></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">There is a </span><span class="c6 c22">huge volume</span><span class="c6">&nbsp;</span><span class="c6">of work being done for Autonomous Vehicles that we largely didn’t touch. However, for those wishing to delve into general market trends, there’s an excellent piece by Moritz Mueller-Freitag of Twenty Billion Neurons about the German auto industry and the impact of autonomous vehicles.</span><sup class="c6 c12"><a href="#ftnt191" id="ftnt_ref191">[191]</a></sup></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Other interesting areas: Image Retrieval/Search,</span><sup class="c6 c12"><a href="#ftnt192" id="ftnt_ref192">[192]</a></sup><span class="c5">&nbsp;Gesture Recognition, Inpainting and Facial Reconstruction.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c5">There is considerable work around Digital Imaging and Communications in Medicine (DICOM) and other medical applications, especially related to imaging. For instance, there have been (and still are) numerous Kaggle detection competitions (lung cancer, cervical cancer), some with large monetary incentives, in which algorithms attempt to outperform specialists at the classification/detection tasks in question. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">However, while work continues on improving the error rates of these algorithms their value as a tool for medical practitioners appears increasingly evident. This is </span><span class="c6 c22">particularly striking</span><span class="c6">&nbsp;when we consider the performance improvements in breast cancer detection achieved by </span><span class="c6 c22">combining AI systems</span><sup class="c6 c12"><a href="#ftnt193" id="ftnt_ref193">[193]</a></sup><span class="c6">&nbsp;with </span><span class="c6 c22">medical specialists</span><span class="c6">.</span><sup class="c6 c12"><a href="#ftnt194" id="ftnt_ref194">[194]</a></sup><span class="c5">&nbsp;In this instance, robot-human symbiosis produces accuracy far greater than the sum of its parts at 99.5%.</span>
</p>

<p class="c1 c2 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">This is just one example of the torrent of medical applications currently being pursued by the deep learning/machine learning communities. Some cynical members of our team jokingly make light of these attempts as a means to ingratiate society to the idea of AI research as a ubiquitous, benevolent force. But as long as the technology helps the healthcare industry, and it is introduced in a safe and considered manner, we wholeheartedly welcome such advances.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c20 c12 c6">Hardware/markets</span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Growing markets for Robotic Vision/Machine Vision (separate fields) and potential target markets for IoT. A personal favourite of ours is the use of Deep Learning, a Raspberry Pi and TensorFlow by a farmer’s son to sort cucumbers in Japan based on unique producer heuristics for quality, e.g. shape, size and colour.</span><sup class="c6 c12"><a href="#ftnt195" id="ftnt_ref195">[195]</a></sup><span class="c6">&nbsp;</span><span class="c5">This produced massive decreases in human-time spent by his mother sorting cucumbers.</span></li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">The trend of shrinking compute requirements and migrating to mobile is evident, &nbsp;but it’s also complemented by steep hardware acceleration. Soon we’ll see pocket sized CNNs and Vision Processing Units (VPUs) everywhere. For instance, the Movidius Myriad2 is used in Google’s Project Tango and drones.</span><sup class="c6 c12"><a href="#ftnt196" id="ftnt_ref196">[196]</a></sup><span class="c6">&nbsp;</span>
    </li>
</ul>

    <div class="embed-container">
    <iframe class="utube-embed utube-center" src="https://www.youtube.com/embed/hX0UELNRR1I">
</iframe>
        </div>
<p class="c1 c2"><span class="c6">The Movidius Fathom stick,</span><sup class="c6 c12"><a href="#ftnt197" id="ftnt_ref197">[197]</a></sup><span class="c6">&nbsp;which also uses the Myriad2’s technology, allows users to add SOTA Computer Vision performance to consumer devices. </span><span class="c5">The Fathom stick, which has the physical properties of a USB stick, brings the power of a Neural Network to almost any device: Brains on a stick.</span></p>

<p class="c1 c2 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c5">Sensors and systems that use something other than visible light. Examples include radar, thermographic cameras, hyperspectral imaging, sonar, magnetic resonance imaging, etc.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Reduction in cost of LIDAR, which use light and radar to measure distances, and offer many advantages over normal RGB cameras. There are many LIDAR devices for </span><span class="c6">currently</span><span class="c6">&nbsp;</span><span class="c6">less than</span><span class="c5">&nbsp;$500.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c6">Hololens and the near-countless other Augmented Reality headsets</span><sup class="c6 c12"><a href="#ftnt198" id="ftnt_ref198">[198]</a></sup><span class="c5">&nbsp;entering the market.</span></li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c12 c6">Project Tango by Google</span><sup class="c12 c6"><a href="#ftnt199" id="ftnt_ref199">[199]</a></sup><span class="c5">&nbsp;represents the next big commercialisation of SLAM. Tango is an augmented reality computing platform, comprising both novel software and hardware. Tango allows the detection of mobile device position, relative to the world, without the use of GPS or other external information while simultaneously mapping the area around the device in 3D. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c6">Corporate partners Lenovo brought affordable Tango enabled phones to market in 2016, allowing hundreds of developers to begin creating applications for the platform. Tango employs the following software</span><span class="c28 c6">&nbsp;</span><span class="c5">technologies: Motion Tracking, Area Learning, and Depth Perception.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c20 c12 c6">Omissions based on forthcoming publications</span></p>

<p class="c1"><span class="c5">There is also considerable, and increasing overlap between Computer Vision techniques and other domains in Machine Learning and Artificial Intelligence. These other domains and hybrid use cases are the subject of The M Tank’s forthcoming publications and, as with the whole of this piece, we partitioned content based on our own heuristics. </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">For instance, we decided to place the two integral Computer Vision tasks, Image Captioning</span><span class="c28 c6">&nbsp;</span><span class="c5">and Visual Question Answering, in our forthcoming NLP piece along with Visual Speech Recognition because of the combination of CV and NLP involved. Whereas the application of Generative Models to images we place in our work on Generative Models. Examples included in these future works are: </span>
</p>

<p class="c1 c9"><span class="c5"></span></p>
<ul class="c18 lst-kix_olf35kpnlxqs-0 start">
    <li class="c1 c2 c13"><span class="c12 c6">Lip Reading</span><span class="c6">: In 2016 we saw huge lip reading advancements in programs such as LipNet</span><sup class="c6 c12"><a href="#ftnt200" id="ftnt_ref200">[200]</a></sup><span class="c6">, which combine Computer Vision and NLP into </span><span class="c6 c22">Visual Speech Recognition</span><span class="c6">. </span></li>
</ul>
<p class="c1 c9"><span class="c29 c28 c6"></span></p>
<ul class="c18 lst-kix_1yf6lzm6ivvf-0">
    <li class="c1 c2 c13"><span class="c12 c6">Generative models</span><span class="c5">&nbsp;applied to images will feature as part of our depiction of the violent* battle between the Autoregressive Models (PixelRNN, PixelCNN, ByteNet, VPN, WaveNet), Generative Adversarial Networks (GANs), Variational Autoencoders and, as you should expect by this stage, all of their variants, combinations and hybrids. </span>
    </li>
</ul>
<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c2"><span class="c3">*Disclaimer: The team wishes to mention that they </span><span class="c3 c22">do not condone</span><span class="c3">&nbsp;Network on Network (NoN) violence in any form and are sympathisers to the movement towards Generative Unadversarial Networks (GUNs).</span><sup class="c6 c12"><a href="#ftnt201" id="ftnt_ref201">[201]</a></sup><span class="c8 c3"><br></span></p>

    <br>

<p class="c1"><span class="c6">In the final section, we’ll offer some concluding remarks and a recapitulation of some of the trends we identified. We would hope that we were comprehensive enough to show a bird’s-eye view</span><span class="c6">&nbsp;of where the Computer Vision field is loosely situated and where it is headed in the near-term. We also would like to draw particular attention to the fact that our work </span><span class="c6 c22">does not cover</span><span class="c6">&nbsp;January-August 2017. The blistering pace of research output means that much of this work could be outdated already; we encourage readers to go and find out whether it is for themselves. </span><span class="c5">But this rapid pace of growth also brings with it lucrative opportunities as the Computer Vision hardware and software markets are expected to reach $48.6 Billion by 2022.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c10"><span class="c12 c6">Figure 20</span><span class="c6">: Computer Vision Revenue by Application Market</span><sup class="c6 c12"><a href="#ftnt202" id="ftnt_ref202">[202]</a></sup></p>

<p class="c10 image-margin" style="margin-top: -2px;"><span class="c6">&nbsp;</span><span style="/*overflow: hidden; display: inline-block;*/ margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 543.78px; height: 313.00px;"><img alt="" class="border cntr" src="images/c-image9.jpg" style="max-width: 624px; width: 95%; /*width: 559.29px; height: 414.42px;/* margin-left: -9.29px; margin-top: -76.86px;*/ transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p>

<p class="c1"><span class="c3 c12">Note</span><span class="c3">: Estimation of Computer Vision revenue by application market spanning the period from 2015-2022. The largest growth is forecasted to come from applications within the automotive, consumer, robotics and machine vision sectors. &nbsp;</span>
    <br>
    <span class="c3 c12">Source</span><span class="c3">: Tractica (2016)</span><sup class="c6 c12"><a href="#ftnt203" id="ftnt_ref203">[203]</a></sup><span class="c6"><br></span></p>

<h1 class="c15 main-heading" id="heading-conclusion"><span class="c25">Conclusion</span></h1>

<p class="c1"><span class="c6">In conclusion we’d like to highlight some of the trends and recurring themes that cropped up repeatedly throughout our research review process. First and foremost, we’d like to draw attention to the Machine Learning research community’s voracious pursuit of optimisation. This is most notable in the year on year changes in accuracy rates, but especially in the </span><span class="c6 c22">intra-year changes in accuracy</span><span class="c5">. We’d like to underscore this point and return to it in a moment.<br><br>Error rates are not the only fanatically optimised parameter, with researchers working on improving speed, efficiency and even the algorithm’s ability to generalise to other tasks and problems in completely new ways. We are acutely aware of the research coming to the fore with approaches like one-shot learning, generative modelling, transfer learning and, as of recently, evolutionary learning, and we feel that these research principles are gradually exerting greater influence on the approaches of the best performing work.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">While this last point is unequivocally meant in commendation for, rather than denigration of, this trend, one can’t help but to cast their mind toward the (very) distant spectre of Artificial General Intelligence, whether merited a thought or not. Far from being alarmist, we just wish to highlight to both experts and laypersons that this concern arises from here, from the startling progress that’s already evident in Computer Vision and other AI subfields. Properly articulated concerns from the public can only come through education about these advancements and their impacts in general. This may then in turn quell the power of media sentiment and misinformation in AI.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">We chose to focus on a one year timeline for two reasons. The first relates to the sheer volume of work being produced. </span><span class="c5">Even for people who follow the field very closely, it is becoming increasingly difficult to remain abreast of research as the number of publications grow exponentially. The second brings us back to our point on intra-year changes.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">In</span><span class="c6">&nbsp;taking a single year snapshot of progress, the reader can begin to comprehend the pace of research at present. We see improvement after improvement in such short time spans, but why? Researchers have cultivated a global community where building on previous approaches (architectures, meta-architectures, techniques, ideas, </span><span class="c6">tips,</span><span class="c28 c6">&nbsp;</span><span class="c6">wacky hacks, results, etc.), and infrastructures (libraries like </span><span class="c6">Keras, TensorFlow and PyTorch, </span><span class="c5">GPUs, etc.), is not only encouraged but also celebrated. A predominantly open source community with few parallels, which is continuously attracting new researchers and having its techniques reappropriated by fields like economics, physics and countless others.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c5">It’s important to understand for those who have yet to notice, that among the already frantic chorus of divergent voices proclaiming divine insight into the true nature of this technology, there is at least agreement; agreement that this technology will alter the world in new and exciting ways. However, much disagreement still comes over the timeline on which these alterations will unravel.</span>
</p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">Until such a time as we can accurately model the progress of these developments we will continue to provide information to the best of our abilities. With this resource we hoped to cater to the spectrum of AI experience, from researchers playing catch-up to anyone who simply wishes to obtain a grounding in Computer Vision and Artificial Intelligence.</span><span class="c28 c6">&nbsp;</span><span class="c6">With this our</span><span class="c5">&nbsp;project hopes to have added some value to the open source revolution that quietly hums beneath the technology of a lifetime.</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1"><span class="c6">With thanks,</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"></span></p>

<p class="c1 c9"><span class="c5"><img alt="" src="images/Signature-transparent.png" style="/*width: 624.00px; height: 290.67px;  transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);*/" title="">
</span></p>

<p class="c1 c9"><span class="c5"></span></p>

<div><p class="c9 c57"><span class="c8 c33"></span></p></div>
    <br><br><br>
    <p class="c1"><span class="c5">The M Tank </span></p>
<hr class="c43">







<!-- References-->

<h1 class="c15 main-heading" id="heading-references"><span class="c20 c12 c25">References</span></h1>

    <br>
    <b>Part One</b>
<div><p class="c11"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c3">&nbsp;British Machine Vision Association (BMVA). 2016. What is computer vision? </span><span class="c7 c3">[Online]</span><span class="c3">&nbsp;Available at: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.bmva.org/visionoverview">http://www.bmva.org/visionoverview</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed 21/12/2016]</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c3">&nbsp;Krizhevsky, A., Sutskever, I. and Hinton, G. E. 2012. </span><span class="c3">ImageNet Classification with Deep Convolutional Neural Networks, </span><span class="c7 c3">NIPS 2012: Neural Information Processing Systems</span><span class="c3">, Lake Tahoe, Nevada. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf">http://www.cs.toronto.edu/~kriz/imagenet_classification_with_deep_convolutional.pdf</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c3">&nbsp;Kuhn, T. S. 1962. </span><span class="c7 c3">The Structure of Scientific Revolutions</span><span class="c3">. 4th ed. United States: The University of Chicago Press.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c3">&nbsp;Karpathy, A. 2015. </span><span class="c3 c14">What a Deep Neural Network thinks about your #selfie. </span><span class="c7 c3 c14">[Blog]</span><span class="c3 c14">&nbsp;</span><span class="c7 c3 c14">Andrej Karpathy Blog</span><span class="c3 c14">. Available: </span><span class="c21 c3 c14"><a class="c4 a-word-wrap" href="http://karpathy.github.io/2015/10/25/selfie/">http://karpathy.github.io/2015/10/25/selfie/</a></span><span class="c3 c14 a-word-wrap">&nbsp;[Accessed: 21/12/2016]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref5" id="ftnt5">[5]</a><span class="c3">&nbsp;Quora. 2016. What is a convolutional neural network? </span><span class="c7 c3">[Online]</span><span class="c3">&nbsp;Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.quora.com/What-is-a-convolutional-neural-network">https://www.quora.com/What-is-a-convolutional-neural-network</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 21/12/2016]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref6" id="ftnt6">[6]</a><span class="c3">&nbsp;Stanford University. 2016. </span><span class="c3 c14">Convolutional Neural Networks for Visual Recognition. </span><span class="c7 c3 c14">[Online] CS231n</span><span class="c3 c14">.</span><span class="c7 c3 c45 c14">&nbsp;</span><span class="c3 c14">Available: </span><span class="c21 c3 c14"><a class="c4 a-word-wrap" href="http://cs231n.stanford.edu/">http://cs231n.stanford.edu/</a></span><span class="c3 c14">&nbsp;[Accessed 21/12/2016]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref7" id="ftnt7">[7]</a><span class="c3">&nbsp;Goodfellow et al. 2016. Deep Learning. </span><span class="c7 c3">MIT Press</span><span class="c3">. </span><span class="c7 c3">[Online]</span><span class="c3">&nbsp;</span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.deeplearningbook.org/">http://www.deeplearningbook.org/</a></span><span class="c3 a-word-wrap">&nbsp;[Accessed: 21/12/2016] Note: Chapter 9, Convolutional Networks [Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.deeplearningbook.org/contents/convnets.html">http://www.deeplearningbook.org/contents/convnets.html</a></span><span class="c8 c3 a-word-wrap">]</span></p></div>
<div>
    <p class="c11">
        <a href="#ftnt_ref8" id="ftnt8">[8]</a><span class="c3 c45">&nbsp;</span><span class="c3">Nielsen, M. 2017. Neural Networks and Deep Learning. </span><span class="c7 c3">[Online] EBook</span><span class="c3">. Available: </span>
    <span class="c21 c3"><a class="c4 a-word-wrap" href="http://neuralnetworksanddeeplearning.com/index.html">http://neuralnetworksanddeeplearning.com/index.html</a></span>
        <span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 06/03/2017].</span>
    </p>
</div>
<div><p class="c11"><a href="#ftnt_ref9" id="ftnt9">[9]</a><span class="c3">&nbsp;ImageNet refers to a popular image dataset for Computer Vision. Each year entrants compete in a series of different tasks called the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/LSVRC/2016/index">http://image-net.org/challenges/LSVRC/2016/index</a></span><span class="c3">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref10" id="ftnt10">[10]</a><span class="c3">&nbsp;See “</span><span class="c7 c3 c14">What I learned from competing against a ConvNet on ImageNet</span><span class="c3 c14">” by Andrej Karpathy. The blog post details the author’s journey to provide a human benchmark against the ILSVRC 2014 dataset. The error rate was approximately 5.1% versus a then state-of-the-art GoogLeNet classification error of 6.8%. Available: </span><span class="c21 c3 c14"><a class="c4 a-word-wrap" href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/</a></span><span class="c3 c14">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref11" id="ftnt11">[11]</a><span class="c8 c3 a-word-wrap">&nbsp;See new datasets later in this piece. </span></p></div>
<div><p class="c11"><a href="#ftnt_ref12" id="ftnt12">[12]</a><span class="c3">&nbsp;Keras is a popular neural network-based deep learning library: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://keras.io/">https://keras.io/</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref13" id="ftnt13">[13]</a><span class="c3">&nbsp;Chollet, F. 2016. </span><span class="c3 c14">Information-theoretical label embeddings for large-scale image classification. </span><span class="c7 c3 c14">[Online] arXiv: 1607.05691</span><span class="c3 c14">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.05691v1">arXiv:1607.05691v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref14" id="ftnt14">[14]</a><span class="c3">&nbsp;Chollet, F. 2016. Xception: Deep Learning with Depthwise Separable Convolutions. </span><span class="c7 c3">[Online]</span><span class="c3">&nbsp;</span><span class="c7 c3">arXiv:1610.02357</span><span class="c3">. Available: </span><span class="c14 c24">&nbsp;</span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1610.02357v2">arXiv:1610.02357v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref15" id="ftnt15">[15]</a><span class="c3">&nbsp;Places2 dataset, details available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://places2.csail.mit.edu/">http://places2.csail.mit.edu/</a></span><span class="c3">. See also new datasets section.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref16" id="ftnt16">[16]</a><span class="c3">&nbsp;Hikvision. 2016. Hikvision ranked No.1 in Scene Classification at ImageNet 2016 challenge. </span><span class="c7 c3">[Online] Security News Desk</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/">http://www.securitynewsdesk.com/hikvision-ranked-no-1-scene-classification-imagenet-2016-challenge/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 20/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref17" id="ftnt17">[17]</a><span class="c8 c3 a-word-wrap">&nbsp;See Residual Networks in Part Four of this publication for more details.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref18" id="ftnt18">[18]</a><span class="c3">&nbsp;Details available under team information Trimps-Soushen from: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/LSVRC/2016/results">http://image-net.org/challenges/LSVRC/2016/results</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref19" id="ftnt19">[19]</a><span class="c3">&nbsp;Xie, S., Girshick, R., Dollar, P., Tu, Z. &amp; He, K. 2016. </span><span class="c3 c14">Aggregated Residual Transformations for Deep Neural Networks. </span><span class="c7 c3 c14">[Online]</span><span class="c3 c14">&nbsp;</span><span class="c7 c3 c14">arXiv: 1611.05431</span><span class="c3 c14">. Available: </span><span class="c12 c14 c17"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.05431v1">arXiv:1611.05431v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref20" id="ftnt20">[20]</a><span class="c3">&nbsp;ImageNet Large Scale Visual Recognition Challenge (2016), Part II, Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/LSVRC/2016/%23det">http://image-net.org/challenges/LSVRC/2016/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 22/11/2016]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref21" id="ftnt21">[21]</a><span class="c3">&nbsp;Hu and Ramanan. 2016. Finding Tiny Faces. </span><span class="c7 c3">[Online] arXiv: 1612.04402. </span><span class="c3">Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.04402v1">arXiv:1612.04402v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref22" id="ftnt22">[22]</a><span class="c3">&nbsp;</span><span class="c3">Liu et al. 2016. </span><span class="c3 c14">SSD: Single Shot MultiBox Detector. </span><span class="c7 c3 c14">[Online] arXiv: 1512.02325v5</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1512.02325v5">arXiv:1512.02325v5</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref23" id="ftnt23">[23]</a><span class="c3">&nbsp;Redmon, J. Farhadi, A. 2016. YOLO9000: Better, Faster, Stronger. </span><span class="c7 c3">[Online] arXiv: 1612.08242v1</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.08242v1">arXiv:1612.08242v1</a></span><span class="c3">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref24" id="ftnt24">[24]</a><span class="c8 c3 a-word-wrap">&nbsp;YOLO stands for “You Only Look Once”.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref25" id="ftnt25">[25]</a><span class="c3">&nbsp;Redmon et al. 2016. </span><span class="c3 c14">You Only Look Once: Unified, Real-Time Object Detection. </span><span class="c7 c3 c14">[Online] arXiv: 1506.02640</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1506.02640v5">arXiv:1506.02640v5</a></span><span class="c3 c14">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref26" id="ftnt26">[26]</a><span class="c3">Redmon. 2017. YOLO: Real-Time Object Detection. </span><span class="c7 c3">[Website] pjreddie.com</span><span class="c3">. Available: </span><span class="c21 c7 c3"><a class="c4 a-word-wrap" href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a></span><span class="c3">&nbsp;[Accessed: 01/03/2017]. </span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref27" id="ftnt27">[27]</a><span class="c3">&nbsp;Lin et al. 2016. </span><span class="c3 c14">Feature Pyramid Networks for Object Detection. </span><span class="c7 c3 c14">[Online] arXiv: 1612.03144</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.03144v1">arXiv:1612.03144v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref28" id="ftnt28">[28]</a><span class="c8 c3 a-word-wrap">&nbsp;Facebook’s Artificial Intelligence Research</span></p></div>
<div><p class="c11"><a href="#ftnt_ref29" id="ftnt29">[29]</a><span class="c8 c3 a-word-wrap">&nbsp;Common Objects in Context (COCO) image dataset </span></p></div>
<div><p class="c11"><a href="#ftnt_ref30" id="ftnt30">[30]</a><span class="c3">&nbsp;Dai et al. 2016. </span><span class="c3 c14">R-FCN: Object Detection via Region-based Fully Convolutional Networks. </span><span class="c7 c3 c14">[Online] arXiv: 1605.06409</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.06409v2">arXiv:1605.06409v2</a></span><span class="c3 c14">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref31" id="ftnt31">[31]</a><span class="c3">&nbsp;Huang et al. 2016. </span><span class="c3 c14">Speed/accuracy trade-offs for modern convolutional object detectors. </span><span class="c7 c3 c14">[Online] arXiv: 1611.10012</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.10012v1">arXiv:1611.10012v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref32" id="ftnt32">[32]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref33" id="ftnt33">[33]</a><span class="c3">&nbsp;Wu et al. 2016. </span><span class="c3 c14">SqueezeDet: Unified, Small, Low Power Fully Convolutional Neural Networks for Real-Time Object Detection for Autonomous Driving. </span><span class="c7 c3 c14">[Online] arXiv: 1612.01051</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.01051v2">arXiv:1612.01051v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref34" id="ftnt34">[34]</a><span class="c3">&nbsp;Hong et al. 2016. PVANet: Lightweight Deep Neural Networks for Real-time Object Detection. </span><span class="c7 c3">[Online] arXiv: 1611.08588v2</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.08588v2">arXiv:1611.08588v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref35" id="ftnt35">[35]</a><span class="c3">&nbsp;DeepGlint Official. 2016. DeepGlint CVPR2016. </span><span class="c7 c3">[Online] Youtube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=xhp47v5OBXQ">https://www.youtube.com/watch?v=xhp47v5OBXQ</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 01/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref36" id="ftnt36">[36]</a><span class="c3">&nbsp;COCO - Common Objects in Common. 2016. </span><span class="c7 c3">[Website] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://mscoco.org/">http://mscoco.org/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref37" id="ftnt37">[37]</a><span class="c8 c3 a-word-wrap">&nbsp;ILSRVC results taken from: ImageNet. 2016. Large Scale Visual Recognition Challenge 2016.</span></p>

    <p class="c11"><span class="c3">&nbsp;</span><span class="c7 c3">[Website] Object Detection</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/LSVRC/2016/results">http://image-net.org/challenges/LSVRC/2016/results</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref38" id="ftnt38">[38]</a><span class="c3">&nbsp;COCO Detection Challenge results taken from: COCO - Common Objects in Common. 2016. </span><span class="c3">Detections Leaderboard </span><span class="c7 c3">[Website] mscoco.org</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://mscoco.org/dataset/%23detections-leaderboard">http://mscoco.org/dataset/#detections-leaderboard</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 05/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref39" id="ftnt39">[39]</a><span class="c3">&nbsp;ImageNet. 2016. [Online] </span><span class="c7 c3">Workshop Presentation</span><span class="c3">, Slide 31. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf">http://image-net.org/challenges/talks/2016/ECCV2016_ilsvrc_coco_detection_segmentation.pdf</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 06/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref40" id="ftnt40">[40]</a><span class="c3">&nbsp;Bertinetto et al. 2016. </span><span class="c3 c14">Fully-Convolutional Siamese Networks for Object Tracking. </span><span class="c7 c3 c14">[Online] arXiv: 1606.09549</span><span class="c3 c14">. Available: </span><span class="c21 c3 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.09549v2">https://arxiv.org/abs/1606.09549v2</a></span><span class="c3 c14">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref41" id="ftnt41">[41]</a><span class="c3">&nbsp;Held et al. 2016. </span><span class="c3 c14">Learning to Track at 100 FPS with Deep Regression Networks. </span><span class="c7 c3 c14">[Online] arXiv: 1604.01802</span><span class="c3 c14">. Available: </span><span class="c21 c3 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.01802v2">https://arxiv.org/abs/1604.01802v2</a></span><span class="c3 c14">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref42" id="ftnt42">[42]</a><span class="c3">&nbsp;David Held. 2016. GOTURN - a neural network tracker. </span><span class="c7 c3">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=kMhwXnLgT_I">https://www.youtube.com/watch?v=kMhwXnLgT_I</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref43" id="ftnt43">[43]</a><span class="c3">&nbsp;Gladh et al. 2016. Deep Motion Features for Visual Tracking. </span><span class="c7 c3">[Online] arXiv: 1612.06615</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.06615v1">arXiv:1612.06615v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref44" id="ftnt44">[44]</a><span class="c3">&nbsp;Gaidon et al. 2016. </span><span class="c3 c14">Virtual Worlds as Proxy for Multi-Object Tracking Analysis. </span><span class="c7 c3 c14">[Online] arXiv: 1605.06457</span><span class="c3 c14">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.06457v1">arXiv:1605.06457v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref45" id="ftnt45">[45]</a><span class="c3">&nbsp;Lee et al. 2016.</span><span class="c3">&nbsp;</span><span class="c3 c14">Globally Optimal Object Tracking with Fully Convolutional Networks. </span><span class="c7 c3 c14">[Online] arXiv: 1612.08274</span><span class="c3 c14">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.08274v1">arXiv:1612.08274v1</a></span><span class="c3 c14">&nbsp;</span></p></div>

    <br>
    <b>Part Two</b>
<div><p class="c11"><a href="#ftnt_ref46" id="ftnt46">[46]</a><span class="c3">&nbsp;Pinheiro, Collobert and Dollar. 2015. Learning to Segment Object Candidates. </span><span class="c7 c3">[Online] arXiv: 1506.06204</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1506.06204v2">arXiv:1506.06204v2</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref47" id="ftnt47">[47]</a><span class="c3">&nbsp;Pinheiro et al. 2016. Learning to Refine Object Segments. </span><span class="c7 c3">[Online] arXiv: 1603.08695</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.08695v2">arXiv:1603.08695v2</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref48" id="ftnt48">[48]</a><span class="c3">&nbsp;Zagoruyko, S. 2016. A MultiPath Network for Object Detection. </span><span class="c7 c3">[Online] arXiv: 1604.02135v2. </span><span class="c3">Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.02135v2">arXiv:1604.02135v2</a></span></p>
</div>
<div><p class="c11"><a href="#ftnt_ref49" id="ftnt49">[49]</a><span class="c3">&nbsp;Dollar, P. 2016. Learning to Segment. </span><span class="c7 c3">[Blog] FAIR</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.fb.com/learning-to-segment/">https://research.fb.com/learning-to-segment/</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref50" id="ftnt50">[50]</a><span class="c3">&nbsp;Dollar, P. 2016. Segmenting and refining images with SharpMask. </span><span class="c7 c3">[Online] Facebook Code</span><span class="c3">. Available: </span><span class="c3 c21"><a class="c4 a-word-wrap" href="https://code.facebook.com/posts/561187904071636/segmenting-and-refining-images-with-sharpmask/">https://code.facebook.com/posts/561187904071636/segmenting-and-refining-images-with-sharpmask/</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref51" id="ftnt51">[51]</a><span class="c3">&nbsp;Jampani et al. 2016. Video Propagation Networks. </span><span class="c7 c3">[Online] arXiv: 1612.05478</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.05478v2">arXiv:1612.05478v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref52" id="ftnt52">[52]</a><span class="c3">&nbsp;Chen et al., 2016. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. </span><span class="c7 c3">[Online] arXiv: 1606.00915</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.00915v1">arXiv:1606.00915v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref53" id="ftnt53">[53]</a><span class="c3">&nbsp;Khoreva et al. 2016. Simple Does It: Weakly Supervised Instance and Semantic Segmentation. </span><span class="c7 c3">[Online] arXiv: 1603.07485v2. </span><span class="c3">Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.07485v2">arXiv:1603.07485v2</a></span></p>
</div>
<div><p class="c11"><a href="#ftnt_ref54" id="ftnt54">[54]</a><span class="c3">&nbsp;Jégou et al. 2016. The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation. </span><span class="c7 c3">[Online] arXiv: 1611.09326v2. </span><span class="c3">Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.09326v2">arXiv:1611.09326v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref55" id="ftnt55">[55]</a><span class="c3">&nbsp;Li et al. 2016. Fully Convolutional Instance-aware Semantic Segmentation. </span><span class="c7 c3">[Online] arXiv: 1611.07709v1</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.07709v1">arXiv:1611.07709v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref56" id="ftnt56">[56]</a><span class="c3">&nbsp;Paszke et al. 2016. ENet: A Deep Neural Network Architecture for Real-Time Semantic Segmentation. </span><span class="c7 c3">[Online] arXiv: 1606.02147v1</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4" href="https://arxiv.org/abs/1606.02147v1">arXiv:1606.02147v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref57" id="ftnt57">[57]</a><span class="c3">&nbsp;Vázquez et al. 2016. A Benchmark for Endoluminal Scene Segmentation of Colonoscopy Images. </span><span class="c7 c3">[Online] arXiv: 1612.00799</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1612.00799v1">arXiv:1612.00799v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref58" id="ftnt58">[58]</a><span class="c3">&nbsp;Dolz et al. 2016. 3D fully convolutional networks for subcortical segmentation in MRI: A large-scale study. </span><span class="c7 c3">[Online] arXiv: 1612.03925</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1612.03925v1">arXiv:1612.03925v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref59" id="ftnt59">[59]</a><span class="c3">&nbsp;Alex et al. 2017. Semi-supervised Learning using Denoising Autoencoders for Brain Lesion Detection and Segmentation. </span><span class="c7 c3">[Online] arXiv: 1611.08664</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1611.08664v4">arXiv:1611.08664v4</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref60" id="ftnt60">[60]</a><span class="c3">&nbsp;Mozaffari and Lee. 2016. 3D Ultrasound image segmentation: A Survey. </span><span class="c7 c3">[Online] arXiv: 1611.09811</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1611.09811v1">arXiv:1611.09811v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref61" id="ftnt61">[61]</a><span class="c3">&nbsp;Dasgupta and Singh. 2016. A Fully Convolutional Neural Network based Structured Prediction Approach Towards the Retinal Vessel Segmentation. </span><span class="c7 c3">[Online] arXiv: 1611.02064</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1611.02064v2">arXiv:1611.02064v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref62" id="ftnt62">[62]</a><span class="c3">&nbsp;Yi et al. 2016. 3-D Convolutional Neural Networks for Glioblastoma Segmentation. </span><span class="c7 c3">[Online] arXiv: 1611.04534</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4" href="https://arxiv.org/abs/1611.04534v1">arXiv:1611.04534v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref63" id="ftnt63">[63]</a><span class="c3">&nbsp;Quan et al. 2016. FusionNet: A deep fully residual convolutional neural network for image segmentation in connectomics. </span><span class="c7 c3">[Online] arXiv: 1612.05360</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4" href="https://arxiv.org/abs/1612.05360v2">arXiv:1612.05360v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref64" id="ftnt64">[64]</a><span class="c8 c3 a-word-wrap">&nbsp;Connectomics refers to the mapping of all connections within an organism’s nervous system, i.e. neurons and their connections. &nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref65" id="ftnt65">[65]</a><span class="c3">&nbsp;Champandard, A.J. 2017. Neural Enhance (latest commit 30/11/2016). </span><span class="c7 c3">[Online] Github</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://github.com/alexjc/neural-enhance">https://github.com/alexjc/neural-enhance</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 11/02/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref66" id="ftnt66">[66]</a><span class="c3">&nbsp;Caballero et al. 2016. Real-Time Video Super-Resolution with Spatio-Temporal Networks and Motion Compensation. </span><span class="c7 c3">[Online] arXiv: 1611.05250</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.05250v1">arXiv:1611.05250v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref67" id="ftnt67">[67]</a><span class="c3">&nbsp;Shi et al. 2016. Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network. </span><span class="c7 c3">[Online] arXiv: 1609.05158</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.05158v2">arXiv:1609.05158v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref68" id="ftnt68">[68]</a><span class="c3">&nbsp;Romano et al. 2016. RAISR: Rapid and Accurate Image Super Resolution. </span><span class="c7 c3">[Online] arXiv: 1606.01299</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.01299v3">arXiv:1606.01299v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref69" id="ftnt69">[69]</a><span class="c3">&nbsp; Milanfar, P. 2016. Enhance! RAISR Sharp Images with Machine Learning. </span><span class="c7 c3">[Blog] Google Research Blog</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html">https://research.googleblog.com/2016/11/enhance-raisr-sharp-images-with-machine.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 20/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref70" id="ftnt70">[70]</a><span class="c8 c3 a-word-wrap">&nbsp;Ledig et al. 2017. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. </span><span class="c7 c3">[Online] arXiv: 1609.04802</span></p></div>
<div><p class="c11"><a href="#ftnt_ref71" id="ftnt71">[71]</a><span class="c3">ibid&nbsp;</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.04802v3">arXiv:1609.04802v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref72" id="ftnt72">[72]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref73" id="ftnt73">[73]</a><span class="c3">&nbsp;Sønderby et al. 2016. Amortised MAP Inference for Image Super-resolution. </span><span class="c7 c3">[Online] arXiv: 1610.04490</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1610.04490v1">arXiv:1610.04490v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref74" id="ftnt74">[74]</a><span class="c3">&nbsp;Prisma. 2017. </span><span class="c7 c3">[Website] Prisma</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4" href="https://prisma-ai.com/">https://prisma-ai.com/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 01/04/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref75" id="ftnt75">[75]</a><span class="c3">&nbsp;Artomatix. 2017. </span><span class="c7 c3">[Website] Artomatix</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4" href="https://services.artomatix.com/">https://services.artomatix.com/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 01/04/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref76" id="ftnt76">[76]</a><span class="c3">&nbsp;Gatys et al. 2015. A Neural Algorithm of Artistic Style. </span><span class="c7 c3">[Online] arXiv: 1508.06576</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1508.06576v2">arXiv:1508.06576v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref77" id="ftnt77">[77]</a><span class="c3">&nbsp;Nikulin &amp; Novak. 2016. Exploring the Neural Algorithm of Artistic Style. </span><span class="c7 c3">[Online] arXiv: 1602.07188</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1602.07188v2">arXiv:1602.07188v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref78" id="ftnt78">[78]</a><span class="c3">&nbsp;Ruder et al. 2016. Artistic style transfer for videos. </span><span class="c7 c3">[Online] arXiv: 1604.08610</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.08610v2">arXiv:1604.08610v2</a></span><span class="c3">&nbsp;</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref79" id="ftnt79">[79]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref80" id="ftnt80">[80]</a><span class="c3">&nbsp;Jia and Vajda. 2016. Delivering real-time AI in the palm of your hand. </span><span class="c7 c3">[Online] Facebook Code</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-the-palm-of-your-hand/">https://code.facebook.com/posts/196146247499076/delivering-real-time-ai-in-the-palm-of-your-hand/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 20/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref81" id="ftnt81">[81]</a><span class="c3">&nbsp;Dumoulin et al. 2016. Supercharging Style Transfer. </span><span class="c7 c3">[Online] Google Research Blog</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.googleblog.com/2016/10/supercharging-style-transfer.html">https://research.googleblog.com/2016/10/supercharging-style-transfer.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 20/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref82" id="ftnt82">[82]</a><span class="c3">&nbsp;Dumoulin et al. 2017. A Learned Representation For Artistic Style. </span><span class="c7 c3">[Online] arXiv: 1610.07629</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1610.07629v5">arXiv:1610.07629v5</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref83" id="ftnt83">[83]</a><span class="c3">&nbsp;Zhang et al. 2016. Colorful Image Colorization. </span><span class="c7 c3">[Online] arXiv: 1603.08511</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4" href="https://arxiv.org/abs/1603.08511v5">arXiv:1603.08511v5</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref84" id="ftnt84">[84]</a><span class="c3">&nbsp;Larsson et al. 2016. Learning Representations for Automatic Colorization. </span><span class="c7 c3">[Online] arXiv: 1603.06668</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4" href="https://arxiv.org/abs/1603.06668v2">arXiv:1603.06668v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref85" id="ftnt85">[85]</a><span class="c3">&nbsp;Lizuka, Simo-Serra and Ishikawa. 2016. Let there be Color!: Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification. </span><span class="c7 c3">[Online] ACM Transaction on Graphics (Proc. of SIGGRAPH), 35(4):110</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/">http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref86" id="ftnt86">[86]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref87" id="ftnt87">[87]</a><span class="c3">&nbsp;Varol et al. 2016. Long-term Temporal Convolutions for Action Recognition. </span><span class="c7 c3">[Online] arXiv: 1604.04494</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.04494v1">arXiv:1604.04494v1</a></span><span class="c3">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref88" id="ftnt88">[88]</a><span class="c3">&nbsp;Feichtenhofer et al. 2016. Spatiotemporal Residual Networks for Video Action Recognition. </span><span class="c7 c3">[Online] arXiv: 1611.02155</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.02155v1">arXiv:1611.02155v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref89" id="ftnt89">[89]</a><span class="c3">&nbsp;Vondrick et al. 2016. Anticipating Visual Representations from Unlabeled Video. </span><span class="c7 c3">[Online] arXiv: 1504.08023</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1504.08023v2">arXiv:1504.08023v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref90" id="ftnt90">[90]</a><span class="c3">&nbsp;Conner-Simons, A., Gordon, R. 2016. Teaching machines to predict the future. </span><span class="c7 c3">[Online] MIT NEWS</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621">https://news.mit.edu/2016/teaching-machines-to-predict-the-future-0621</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/02/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref91" id="ftnt91">[91]</a><span class="c3">&nbsp;Idrees et al. 2016. The THUMOS Challenge on Action Recognition for Videos "in the Wild". </span><span class="c7 c3">[Online] arXiv: 1604.06182</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.06182v1">arXiv:1604.06182v1</a></span>
</p></div>

    <br>
    <b>Part Three</b>
    <div><p class="c11"><a href="#ftnt_ref92" id="ftnt92">[92]</a><span class="c3">&nbsp;Rezende et al. 2016. Unsupervised Learning of 3D Structure from Images. </span><span class="c7 c3">[Online] arXiv: 1607.00662</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.00662v1">arXiv:1607.00662v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref93" id="ftnt93">[93]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref94" id="ftnt94">[94]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref95" id="ftnt95">[95]</a><span class="c8 c3 a-word-wrap">&nbsp;Pose Estimation can refer to either just an object’s orientation, or both orientation and position in 3D space.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref96" id="ftnt96">[96]</a><span class="c3">&nbsp;Riegler et al. 2016. OctNet: Learning Deep 3D Representations at High Resolutions. </span><span class="c7 c3">[Online] arXiv: 1611.05009</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.05009v3">arXiv:1611.05009v3</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref97" id="ftnt97">[97]</a><span class="c3">&nbsp;Xiang et al. 2016. ObjectNet3D: A Large Scale Database for 3D Object Recognition. </span><span class="c7 c3">[Online] Computer Vision and Geometry Lab, Stanford University (cvgl.stanford.edu)</span><span class="c3">. Available from: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://cvgl.stanford.edu/projects/objectnet3d/">http://cvgl.stanford.edu/projects/objectnet3d/</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref98" id="ftnt98">[98]</a><span class="c3">&nbsp;Choy et al. 2016. 3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. </span><span class="c7 c3">[Online] arXiv: 1604.00449</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.00449v1">arXiv:1604.00449v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref99" id="ftnt99">[99]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref100" id="ftnt100">[100]</a><span class="c3">&nbsp;Gadelha et al. 2016. 3D Shape Induction from 2D Views of Multiple Objects. </span><span class="c7 c3">[Online] arXiv: 1612.058272</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.05872v1">arXiv:1612.05872v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref101" id="ftnt101">[101]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref102" id="ftnt102">[102]</a><span class="c3">&nbsp;Rezende et al. 2016. Unsupervised Learning of 3D Structure from Images. </span><span class="c7 c3">[Online] arXiv: 1607.00662</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.00662v1">arXiv:1607.00662v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref103" id="ftnt103">[103]</a><span class="c3">&nbsp;Colyer, A. 2017. Unsupervised learning of 3D structure from images. </span><span class="c7 c3">[Blog] the morning paper</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://blog.acolyer.org/2017/01/05/unsupervised-learning-of-3d-structure-from-images/">https://blog.acolyer.org/2017/01/05/unsupervised-learning-of-3d-structure-from-images/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref104" id="ftnt104">[104]</a><span class="c3">&nbsp;COCO. 2016. Welcome to the COCO 2016 Keypoint Challenge! </span><span class="c7 c3">[Online] Common Objects in Common (mscoco.org)</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://mscoco.org/dataset/%23keypoints-challenge2016">http://mscoco.org/dataset/#keypoints-challenge2016</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 27/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref105" id="ftnt105">[105]</a><span class="c3">&nbsp;ECCV. 2016. Webpage. </span><span class="c7 c3">[Online] European Convention on Computer Vision (</span><span class="c21 c7 c3"><a class="c4" href="http://www.eccv2016.org">www.eccv2016.org</a></span><span class="c7 c3">)</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.eccv2016.org/main-conference/">http://www.eccv2016.org/main-conference/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 26/01/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref106" id="ftnt106">[106]</a><span class="c3">&nbsp;Cao et al. 2016. Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. </span><span class="c7 c3">[Online] arXiv: 161108050</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.08050v1">arXiv:1611.08050v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref107" id="ftnt107">[107]</a><span class="c3">&nbsp;Zhe Cao. 2016. Realtime Multi-Person 2D Human Pose Estimation using Part Affinity Fields, CVPR 2017 Oral. </span><span class="c3 c7">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=pW6nZXeWlGM">https://www.youtube.com/watch?v=pW6nZXeWlGM</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref108" id="ftnt108">[108]</a><span class="c3">&nbsp;Bogo et al. 2016. Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image. </span><span class="c7 c3">[Online] arXiv: 1607.08128</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.08128v1">arXiv:1607.08128v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref109" id="ftnt109">[109]</a><span class="c3">&nbsp;Michael Black. 2016. SMPLify: 3D Human Pose and Shape from a Single Image (ECCV 2016). </span><span class="c7 c3">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=eUnZ2rjxGaE">https://www.youtube.com/watch?v=eUnZ2rjxGaE</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref110" id="ftnt110">[110]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref111" id="ftnt111">[111]</a><span class="c3">&nbsp;Dou et al. 2016. Fusion4D: Real-time Performance Capture of Challenging Scenes. </span><span class="c7 c3">[Online] &nbsp;SamehKhamis.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.samehkhamis.com/dou-siggraph2016.pdf">http://www.samehkhamis.com/dou-siggraph2016.pdf</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref112" id="ftnt112">[112]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref113" id="ftnt113">[113]</a><span class="c3">&nbsp;Microsoft Research. 2016. Fusion4D: Real-time Performance Capture of Challenging Scenes. </span><span class="c7 c3">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=2dkcJ1YhYw4%26feature%3Dyoutu.be">https://www.youtube.com/watch?v=2dkcJ1YhYw4&amp;feature=youtu.be</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref114" id="ftnt114">[114]</a><span class="c3">&nbsp;I3D Past Projects. 2016. holoportation: virtual 3D teleportation in real-time (Microsoft Research). </span><span class="c7 c3">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=7d59O6cfaM0%26feature%3Dyoutu.be">https://www.youtube.com/watch?v=7d59O6cfaM0&amp;feature=youtu.be</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref115" id="ftnt115">[115]</a><span class="c3">&nbsp;Kim et al. 2016. Real-Time 3D Reconstruction and 6-DoF Tracking with an Event Camera. </span><span class="c7 c3">[Online] Department of Computer, Imperial College London (</span><span class="c21 c7 c3"><a class="c4 a-word-wrap" href="http://www.doc.ic.ac.uk">www.doc.ic.ac.uk</a></span><span class="c7 c3">)</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_eccv2016.pdf">https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_eccv2016.pdf</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref116" id="ftnt116">[116]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref117" id="ftnt117">[117]</a><span class="c3">&nbsp;Kim et al. 2014. Simultaneous Mosaicing and Tracking with an Event Camera. </span><span class="c7 c3">[Online] Department of Computer, Imperial College London (</span><span class="c21 c7 c3"><a class="c4 a-word-wrap" href="http://www.doc.ic.ac.uk">www.doc.ic.ac.uk</a></span><span class="c7 c3">). </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_bmvc2014.pdf">https://www.doc.ic.ac.uk/~ajd/Publications/kim_etal_bmvc2014.pdf</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref118" id="ftnt118">[118]</a><span class="c3">&nbsp;Hanme Kim. 2017. Real-Time 3D Reconstruction and 6-DoF Tracking with an Event. </span><span class="c7 c3">[Online] YouTube.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.youtube.com/watch?v=yHLyhdMSw7w">https://www.youtube.com/watch?v=yHLyhdMSw7w</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref119" id="ftnt119">[119]</a><span class="c3">&nbsp;Garg et al. 2016. Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue. </span><span class="c7 c3">[Online] arXiv: 1603.04992</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.04992v2">arXiv:1603.04992v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref120" id="ftnt120">[120]</a><span class="c3">&nbsp;Izadinia et al. 2016. IM2CAD. </span><span class="c7 c3">[Online] arXiv: 1608.05137</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1608.05137v1">arXiv:1608.05137v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref121" id="ftnt121">[121]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref122" id="ftnt122">[122]</a><span class="c8 c3 a-word-wrap">&nbsp;Yet more neural network spillover</span></p></div>
<div><p class="c11"><a href="#ftnt_ref123" id="ftnt123">[123]</a><span class="c3">&nbsp;Tokmakov et al. 2016. Learning Motion Patterns in Videos. </span><span class="c7 c3">[Online] arXiv: 1612.07217</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.07217v1">arXiv:1612.07217v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref124" id="ftnt124">[124]</a><span class="c3">DAVIS. 2017. DAVIS: Densely Annotated Video Segmentation. </span><span class="c7 c3">[Website] DAVIS Challenge</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://davischallenge.org/">http://davischallenge.org/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 27/03/2017].</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref125" id="ftnt125">[125]</a><span class="c3">&nbsp;DeTone et al. 2016. Deep Image Homography Estimation. </span><span class="c7 c3">[Online] arXiv: 1606.03798</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.03798v1">arXiv:1606.03798v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref126" id="ftnt126">[126]</a><span class="c3">&nbsp;Handa et al. 2016. gvnn: Neural Network Library for Geometric Computer Vision. </span><span class="c7 c3">[Online] arXiv: 1607.07405</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.07405v3">arXiv:1607.07405v3</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref127" id="ftnt127">[127]</a><span class="c3">&nbsp;Malisiewicz. 2016. The Future of Real-Time SLAM and Deep Learning vs SLAM. </span><span class="c7 c3">[Blog] Tombone's Computer Vision Blog</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html">http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 01/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref128" id="ftnt128">[128]</a><span class="c3">&nbsp;Google. 2017. Tango. </span><span class="c7 c3">[Website] get.google.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4" href="https://get.google.com/tango/">https://get.google.com/tango/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 23/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref129" id="ftnt129">[129]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref130" id="ftnt130">[130]</a><span class="c3">&nbsp;Malisiewicz. 2016. The Future of Real-Time SLAM and Deep Learning vs SLAM. </span><span class="c7 c3">[Blog] Tombone's Computer Vision Blog</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html">http://www.computervisionblog.com/2016/01/why-slam-matters-future-of-real-time.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 01/03/2017].</span></p></div>


    <br>
    <b>Part Four</b>
    <div><p class="c11"><a href="#ftnt_ref131" id="ftnt131">[131]</a><span class="c3">&nbsp;Szegedy et al. 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. </span><span class="c7 c3">[Online] arXiv: 1602.07261</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1602.07261v2">arXiv:1602.07261v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref132" id="ftnt132">[132]</a><span class="c3">&nbsp;Szegedy et al. 2015. Rethinking the Inception Architecture for Computer Vision. </span><span class="c7 c3">[Online] arXiv: 1512.00567</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1512.00567v3">arXiv:1512.00567v3</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref133" id="ftnt133">[133]</a><span class="c3">&nbsp;Huang et al. 2016. Densely Connected Convolutional Networks. </span><span class="c7 c3">[Online] arXiv: 1608.06993</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1608.06993v3">arXiv:1608.06993v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref134" id="ftnt134">[134]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref135" id="ftnt135">[135]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref136" id="ftnt136">[136]</a><span class="c3">&nbsp;Liuzhuang13. 2017. Code for Densely Connected Convolutional Networks (DenseNets). </span><span class="c7 c3">[Online] github.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://github.com/liuzhuang13/DenseNet">https://github.com/liuzhuang13/DenseNet</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/04/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref137" id="ftnt137">[137]</a><span class="c3">&nbsp;Larsson et al. 2016. FractalNet: Ultra-Deep Neural Networks without Residuals. </span><span class="c7 c3">[Online] arXiv: 1605.07648</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.07648v2">arXiv:1605.07648v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref138" id="ftnt138">[138]</a><span class="c3">&nbsp;Huang et al. 2016. Densely Connected Convolutional Networks. </span><span class="c7 c3">[Online] arXiv: 1608.06993</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1608.06993v3">arXiv:1608.06993v3</a></span><span class="c3">, pg. 1.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref139" id="ftnt139">[139]</a><span class="c3">&nbsp;Hossein HasanPour et al. 2016. Lets keep it simple: using simple architectures to outperform deeper architectures. </span><span class="c7 c3">[Online] arXiv: 1608.06037</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1608.06037v3">arXiv:1608.06037v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref140" id="ftnt140">[140]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref141" id="ftnt141">[141]</a><span class="c3">&nbsp;Singh et al. 2016. Swapout: Learning an ensemble of deep architectures. </span><span class="c7 c3">[Online] arXiv: 1605.06465</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.06465v1">arXiv:1605.06465v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref142" id="ftnt142">[142]</a><span class="c3">&nbsp;Iandola et al. 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size. </span><span class="c7 c3">[Online] arXiv: 1602.07360</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1602.07360v4">arXiv:1602.07360v4</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref143" id="ftnt143">[143]</a><span class="c3">&nbsp;Shang et al. 2016. Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units.</span><span class="c7 c3">&nbsp;[Online] arXiv: 1603.05201</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.05201v2">arXiv:1603.05201v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref144" id="ftnt144">[144]</a><span class="c3">&nbsp;Clevert et al. 2016. Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs). </span><span class="c7 c3">[Online] arXiv: 1511.07289</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1511.07289v5">arXiv:1511.07289v5</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref145" id="ftnt145">[145]</a><span class="c3">&nbsp;Trottier et al. 2016. Parametric Exponential Linear Unit for Deep Convolutional Neural Networks. </span><span class="c7 c3">[Online] arXiv: 1605.09332</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.09332v3">arXiv:1605.09332v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref146" id="ftnt146">[146]</a><span class="c3">&nbsp;Worrall et al. 2016. Harmonic Networks: Deep Translation and Rotation Equivariance. </span><span class="c7 c3">[Online] arXiv: 1612.04642</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.04642v1">arXiv:1612.04642v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref147" id="ftnt147">[147]</a><span class="c3">&nbsp;Cohen &amp; Welling. 2016. Group Equivariant Convolutional Networks. </span><span class="c7 c3">[Online] arXiv: 1602.07576</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1602.07576v3">arXiv:1602.07576v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref148" id="ftnt148">[148]</a><span class="c3">&nbsp;Dieleman et al. 2016. Exploiting Cyclic Symmetry in Convolutional Neural Networks. </span><span class="c7 c3">[Online] arXiv: 1602.02660</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1602.02660v2">arXiv:1602.02660v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref149" id="ftnt149">[149]</a><span class="c3">&nbsp;Cohen &amp; Welling. 2016. Steerable CNNs. </span><span class="c7 c3">[Online] arXiv: 1612.08498</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.08498v1">arXiv:1612.08498v1</a></span><span class="c3">&nbsp;</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref150" id="ftnt150">[150]</a><span class="c3">&nbsp;Abdi, M., Nahavandi, S. 2016. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. </span><span class="c7 c3">[Online] arXiv: 1609.05672</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.05672v3">arXiv:1609.05672v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref151" id="ftnt151">[151]</a><span class="c3">&nbsp;He et al. 2015. Deep Residual Learning for Image Recognition. </span><span class="c7 c3">[Online] arXiv: 1512.03385</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4" href="https://arxiv.org/abs/1512.03385v1">arXiv:1512.03385v1</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref152" id="ftnt152">[152]</a><span class="c3">&nbsp;Quora. 2017. What is an intuitive explanation of Deep Residual Networks? </span><span class="c7 c3">[Website] </span><span class="c21 c7 c3"><a class="c4" href="http://www.quora.com">www.quora.com</a></span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks">https://www.quora.com/What-is-an-intuitive-explanation-of-Deep-Residual-Networks</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/04/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref153" id="ftnt153">[153]</a><span class="c3">&nbsp;Zagoruyko, S. and Komodakis, N. 2017. Wide Residual Networks. </span><span class="c7 c3">[Online] arXiv: 1605.07146</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.07146v3">arXiv:1605.07146v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref154" id="ftnt154">[154]</a><span class="c3">&nbsp;Huang et al. 2016. Deep Networks with Stochastic Depth. </span><span class="c7 c3">[Online] arXiv: 1603.09382</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.09382v3">arXiv:1603.09382v3</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref155" id="ftnt155">[155]</a><span class="c3">&nbsp;Savarese et al. 2016. Learning Identity Mappings with Residual Gates. </span><span class="c7 c3">[Online] arXiv: 1611.01260</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.01260v2">arXiv:1611.01260v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref156" id="ftnt156">[156]</a><span class="c3">&nbsp;Veit, Wilber and Belongie. 2016. Residual Networks Behave Like Ensembles of Relatively Shallow Networks. </span><span class="c7 c3">[Online] arXiv: 1605.06431</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.06431v2">arXiv:1605.06431v2</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref157" id="ftnt157">[157]</a><span class="c3">&nbsp;He at al. 2016. Identity Mappings in Deep Residual Networks. </span><span class="c7 c3">[Online] arXiv: 1603.05027</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.05027v3">arXiv:1603.05027v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref158" id="ftnt158">[158]</a><span class="c3">&nbsp;Abdi, M., Nahavandi, S. 2016. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. </span><span class="c7 c3">[Online] arXiv: 1609.05672</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.05672v3">arXiv:1609.05672v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref159" id="ftnt159">[159]</a><span class="c3">&nbsp;Greff et al. 2017. Highway and Residual Networks learn Unrolled Iterative Estimation. </span><span class="c7 c3">[Online] arXiv: 1612. 07771</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.07771v3">arXiv:1612.07771v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref160" id="ftnt160">[160]</a><span class="c3">&nbsp;Abdi and Nahavandi. 2017. Multi-Residual Networks: Improving the Speed and Accuracy of Residual Networks. </span><span class="c7 c3">[Online] 1609.05672</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.05672v4">arXiv:1609.05672v4</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref161" id="ftnt161">[161]</a><span class="c3">&nbsp;Targ et al. 2016. Resnet in Resnet: Generalizing Residual Architectures. </span><span class="c7 c3">[Online] arXiv: 1603.08029</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1603.08029v1">arXiv:1603.08029v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref162" id="ftnt162">[162]</a><span class="c3">&nbsp;Wu et al. 2016. Wider or Deeper: Revisiting the ResNet Model for Visual Recognition.</span><span class="c7 c3">&nbsp;[Online] arXiv: 1611.10080</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.10080v1">arXiv:1611.10080v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref163" id="ftnt163">[163]</a><span class="c3">&nbsp;Liao and Poggio. 2016. Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex. </span><span class="c7 c3">[Online] arXiv: 1604.03640</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.03640v1">arXiv:1604.03640v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref164" id="ftnt164">[164]</a><span class="c3">&nbsp;Moniz and Pal. 2016. Convolutional Residual Memory Networks. </span><span class="c7 c3">[Online] arXiv: 1606.05262</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.05262v3">arXiv:1606.05262v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref165" id="ftnt165">[165]</a><span class="c3">&nbsp;Hardt and Ma. 2016. Identity Matters in Deep Learning. </span><span class="c7 c3">[Online] arXiv: 1611.04231</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.04231v2">arXiv:1611.04231v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref166" id="ftnt166">[166]</a><span class="c3">&nbsp;Shah et al. 2016. Deep Residual Networks with Exponential Linear Unit. </span><span class="c7 c3">[Online] arXiv: 1604.04112</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.04112v4">arXiv:1604.04112v4</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref167" id="ftnt167">[167]</a><span class="c3">&nbsp;Shen and Zeng. 2016. Weighted Residuals for Very Deep Networks. </span><span class="c7 c3">[Online] arXiv: 1605.08831</span><span class="c3">. Available: </span><span class="c21 c12 c14 c24"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1605.08831v1">arXiv:1605.08831v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref168" id="ftnt168">[168]</a><span class="c3">&nbsp;Ben Hamner. 2016. Twitter Status. </span><span class="c7 c3">[Online] Twitter.</span><span class="c3">&nbsp;Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://twitter.com/benhamner/status/789909204832227329">https://twitter.com/benhamner/status/789909204832227329</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref169" id="ftnt169">[169]</a><span class="c3">&nbsp;ImageNet. 2017. Homepage. </span><span class="c7 c3">[Online] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://image-net.org/index">http://image-net.org/index</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref170" id="ftnt170">[170]</a><span class="c3">&nbsp;COCO. 2017. Common Objects in Common Homepage. </span><span class="c7 c3">[Online] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://mscoco.org/">http://mscoco.org/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref171" id="ftnt171">[171]</a><span class="c3">&nbsp;CIFARs. 2017. The CIFAR-10 dataset. </span><span class="c7 c3">[Online] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref172" id="ftnt172">[172]</a><span class="c3">&nbsp;MNIST. 2017. THE MNIST DATABASE of handwritten digits. </span><span class="c7 c3">[Online] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/01/2017]</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref173" id="ftnt173">[173]</a><span class="c3">&nbsp;Zhou et al. 2016. Places2. </span><span class="c7 c3">[Online] </span><span class="c3">Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://places2.csail.mit.edu/">http://places2.csail.mit.edu/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 06/01/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref174" id="ftnt174">[174]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref175" id="ftnt175">[175]</a><span class="c3">&nbsp;McCormac et al. 2017. SceneNet RGB-D: 5M Photorealistic Images of Synthetic Indoor Trajectories with Ground Truth. </span><span class="c7 c3">[Online] arXiv: 1612.05079</span><span class="c3">v3. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1612.05079v3">arXiv:1612.05079v3</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref176" id="ftnt176">[176]</a><span class="c3">&nbsp;Aytar et al. 2016. Cross-Modal Scene Networks. </span><span class="c7 c3">[Online] arXiv: 1610.09003</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1610.09003v1">arXiv:1610.09003v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref177" id="ftnt177">[177]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>
<div><p class="c11"><a href="#ftnt_ref178" id="ftnt178">[178]</a><span class="c3">&nbsp;Guo et al. 2016. MS-Celeb-1M: A Dataset and Benchmark for Large-Scale Face Recognition. </span><span class="c7 c3">[Online] arXiv: 1607.08221</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1607.08221v1">arXiv:1607.08221v1</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref179" id="ftnt179">[179]</a><span class="c3">&nbsp;Open Images. 2017. Open Images Dataset. </span><span class="c7 c3">[Online] Github</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://github.com/openimages/dataset">https://github.com/openimages/dataset</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 08/01/2017]</span></p></div>
<div><p class="c11"><a href="#ftnt_ref180" id="ftnt180">[180]</a><span class="c3">&nbsp;Abu-El-Haija et al. 2016. YouTube-8M: A Large-Scale Video Classification Benchmark. </span><span class="c7 c3">[Online] arXiv: 1609.08675</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1609.08675v1">arXiv:1609.08675v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref181" id="ftnt181">[181]</a><span class="c3">&nbsp;Natsev, P. 2017. An updated YouTube-8M, a video understanding challenge, and a CVPR workshop. Oh my!. </span><span class="c7 c3">[Online] Google Research Blog</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.googleblog.com/2017/02/an-updated-youtube-8m-video.html">https://research.googleblog.com/2017/02/an-updated-youtube-8m-video.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 26/02/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref182" id="ftnt182">[182]</a><span class="c3">&nbsp;YouTube-8M. 2017. CVPR'17 Workshop on YouTube-8M Large-Scale Video Understanding. </span><span class="c7 c3">[Online] Google Research</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.google.com/youtube8m/workshop.html">https://research.google.com/youtube8m/workshop.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 26/02/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref183" id="ftnt183">[183]</a><span class="c3">&nbsp;Google. 2017. YouTube-8M Dataset. </span><span class="c7 c3">[Online] research.google.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://research.google.com/youtube8m/">https://research.google.com/youtube8m/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 04/03/2017].</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref184" id="ftnt184">[184]</a><span class="c3">&nbsp;Wu, Pique &amp; Wieland. 2016. Using Artificial Intelligence to Help Blind People ‘See’ Facebook. </span><span class="c7 c3">[Online] Facebook Newsroom</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://newsroom.fb.com/news/2016/04/using-artificial-intelligence-to-help-blind-people-see-facebook/">http://newsroom.fb.com/news/2016/04/using-artificial-intelligence-to-help-blind-people-see-facebook/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 02/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref185" id="ftnt185">[185]</a><span class="c3">&nbsp;Metz. 2016. Artificial Intelligence Finally Entered Our Everyday World. </span><span class="c7 c3">[Online] Wired</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.wired.com/2016/01/2015-was-the-year-ai-finally-entered-the-everyday-world/">https://www.wired.com/2016/01/2015-was-the-year-ai-finally-entered-the-everyday-world/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 02/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref186" id="ftnt186">[186]</a><span class="c3">&nbsp;Doerrfeld. 2015. 20+ Emotion Recognition APIs That Will Leave You Impressed, and Concerned. </span><span class="c7 c3">[Online] Nordic Apis</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/">http://nordicapis.com/20-emotion-recognition-apis-that-will-leave-you-impressed-and-concerned/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 02/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref187" id="ftnt187">[187]</a><span class="c3">&nbsp;Johnson, A. 2016. Trailbehind/DeepOSM - Train a deep learning net with OpenStreetMap features and satellite imagery. </span><span class="c7 c3">[Online] Github.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://github.com/trailbehind/DeepOSM">https://github.com/trailbehind/DeepOSM</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 29/03/2017]. </span></p></div>
<div><p class="c11"><a href="#ftnt_ref188" id="ftnt188">[188]</a><span class="c3">&nbsp;Gros and Tiecke. 2016. Connecting the world with better maps. </span><span class="c7 c3">[Online] Facebook Code</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://code.facebook.com/posts/1676452492623525/connecting-the-world-with-better-maps/">https://code.facebook.com/posts/1676452492623525/connecting-the-world-with-better-maps/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 02/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref189" id="ftnt189">[189]</a><span class="c3">&nbsp;Amazon. 2017. Frequently Asked Questions - Amazon Go. </span><span class="c7 c3">[Website] Amazon.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.amazon.com/b?node%3D16008589011">https://www.amazon.com/b?node=16008589011</a></span><span class="c3">&nbsp;[Accessed: 29/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref190" id="ftnt190">[190]</a><span class="c3">&nbsp;Reisinger, D. 2017. Amazon’s Cashier-Free Store Might Be Easy to Break. </span><span class="c7 c3">[Online] Fortune Tech</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://fortune.com/2017/03/28/amazon-go-cashier-free-store/">http://fortune.com/2017/03/28/amazon-go-cashier-free-store/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 29/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref191" id="ftnt191">[191]</a><span class="c3">&nbsp;Mueller-Freitag, M. 2017. Germany asleep at the wheel? </span><span class="c7 c3">[Blog] Twenty Billion Neurons - Medium.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://medium.com/twentybn/germany-asleep-at-the-wheel-d800445d6da2">https://medium.com/twentybn/germany-asleep-at-the-wheel-d800445d6da2</a></span><span class="c8 c3 a-word-wrap">&nbsp;</span></p></div>
<div><p class="c11"><a href="#ftnt_ref192" id="ftnt192">[192]</a><span class="c3">&nbsp;Gordo et al. 2016. Deep Image Retrieval: Learning global representations for image search. </span><span class="c7 c3">[Online] arXiv: 1604.01325</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1604.01325v2">arXiv:1604.01325v2</a></span><span>&nbsp;</span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref193" id="ftnt193">[193]</a><span class="c3">&nbsp;Wang et al. 2016. Deep Learning for Identifying Metastatic Breast Cancer. </span><span class="c7 c3">[Online] arXiv: 1606.05718</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1606.05718v1">arXiv:1606.05718v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref194" id="ftnt194">[194]</a><span class="c3">&nbsp;Rosenfeld, J. 2016. AI Achieves Near-Human Detection of Breast Cancer. </span><span class="c7 c3">[Online] Mentalfloss.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://mentalfloss.com/article/82415/ai-achieves-near-human-detection-breast-cancer">http://mentalfloss.com/article/82415/ai-achieves-near-human-detection-breast-cancer</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 27/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref195" id="ftnt195">[195]</a><span class="c3">&nbsp;Sato, K. 2016. How a Japanese cucumber farmer is using deep learning and TensorFlow. </span><span class="c7 c3">[Blog] Google Cloud Platform</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow">https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow</a></span>
</p></div>
<div><p class="c11"><a href="#ftnt_ref196" id="ftnt196">[196]</a><span class="c3">&nbsp;Banerjee, P. 2016. The Rise of VPUs: Giving eyes to machines. </span><span class="c7 c3">[Online] </span><span class="c21 c7 c3"><a class="c4 a-word-wrap" href="http://www.digit.in">www.digit.in</a></span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.digit.in/general/the-rise-of-vpus-giving-eyes-to-machines-29561.html">http://www.digit.in/general/the-rise-of-vpus-giving-eyes-to-machines-29561.html</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 22/03/2017.</span></p></div>
<div><p class="c11"><a href="#ftnt_ref197" id="ftnt197">[197]</a><span class="c3">&nbsp;Movidius. 2017. Embedded Neural Network Compute Framework: Fathom. </span><span class="c7 c3">[Online] Movidius.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.movidius.com/solutions/machine-vision-algorithms/machine-learning">https://www.movidius.com/solutions/machine-vision-algorithms/machine-learning</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref198" id="ftnt198">[198]</a><span class="c3">&nbsp;Dzyre, N. 2016. 10 Forthcoming Augmented Reality &amp; Smart Glasses You Can Buy. </span><span class="c7 c3">[Blog]</span><span class="c3">&nbsp;</span><span class="c7 c3">Hongkiat</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="http://www.hongkiat.com/blog/augmented-reality-smart-glasses/">http://www.hongkiat.com/blog/augmented-reality-smart-glasses/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 03/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref199" id="ftnt199">[199]</a><span class="c3">&nbsp;Google. 2017. Tango. </span><span class="c7 c3">[Website] get.google.com</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://get.google.com/tango/">https://get.google.com/tango/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 23/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref200" id="ftnt200">[200]</a><span class="c3">&nbsp;Assael et al. 2016. LipNet: End-to-End Sentence-level Lipreading. </span><span class="c7 c3">[Online] arXiv: 1611.01599</span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1611.01599v2">arXiv:1611.01599v2</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref201" id="ftnt201">[201]</a><span class="c3">&nbsp;Albanie et al. 2017. Stopping GAN Violence: Generative Unadversarial Networks. </span><span class="c7 c3">[Online] arXiv: 1703.02528</span><span class="c3">. Available: </span><span class="c17 c12 c14"><a class="c4 a-word-wrap" href="https://arxiv.org/abs/1703.02528v1">arXiv:1703.02528v1</a></span></p></div>
<div><p class="c11"><a href="#ftnt_ref202" id="ftnt202">[202]</a><span class="c3">&nbsp;Tractica. 2016. Computer Vision Hardware and Software Market to Reach $48.6 Billion by 2022. </span><span class="c7 c3">[Website] </span><span class="c21 c7 c3"><a class="c4 a-word-wrap" href="http://www.tractica.com">www.tractica.com</a></span><span class="c3">. Available: </span><span class="c21 c3"><a class="c4 a-word-wrap" href="https://www.tractica.com/newsroom/press-releases/computer-vision-hardware-and-software-market-to-reach-48-6-billion-by-2022/">https://www.tractica.com/newsroom/press-releases/computer-vision-hardware-and-software-market-to-reach-48-6-billion-by-2022/</a></span><span class="c8 c3 a-word-wrap">&nbsp;[Accessed: 12/03/2017].</span></p></div>
<div><p class="c11"><a href="#ftnt_ref203" id="ftnt203">[203]</a><span class="c8 c3 a-word-wrap">&nbsp;ibid</span></p></div>

</div>